# @package _global_

defaults:
  - dreamer_v1
  - override /algo: p2e_dv1
  - _self_

algo:
  name: p2e_dv1_exploration

# Model Manager
model_manager:
  models: 
    world_model:
      model_name: "${exp_name}_world_model"
      description: "P2E_DV1 World Model used in ${env.id} Environment"
      tags: {}
    ensembles:
      model_name: "${exp_name}_ensembles"
      description: "P2E_DV1 Ensembles used in ${env.id} Environment"
      tags: {}
    actor_exploration:
      model_name: "${exp_name}_actor_exploration"
      description: "P2E_DV1 Actor Exploration used in ${env.id} Environment"
      tags: {}
    critic_exploration:
      model_name: "${exp_name}_critic_exploration"
      description: "P2E_DV1 Critic Exploration used in ${env.id} Environment"
      tags: {}
    actor_task:
      model_name: "${exp_name}_actor_task"
      description: "P2E_DV1 Actor Exploration used in ${env.id} Environment"
      tags: {}
    critic_task:
      model_name: "${exp_name}_critic_task"
      description: "P2E_DV1 Critic Exploration used in ${env.id} Environment"
      tags: {}

metric:
  aggregator:
    metrics:
      Loss/world_model_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/value_loss_task: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss_task: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/value_loss_exploration: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss_exploration: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/observation_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/reward_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/state_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/continue_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/ensemble_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/kl: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/post_entropy: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/prior_entropy: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Params/exploration_amount_task:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Params/exploration_amount_exploration: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Rewards/intrinsic: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Values_exploration/predicted_values: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Values_exploration/lambda_values: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/world_model: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/actor_task: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/critic_task: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/actor_exploration: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/critic_exploration: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/ensemble: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}