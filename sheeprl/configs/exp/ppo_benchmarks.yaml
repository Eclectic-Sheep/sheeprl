# @package _global_

defaults:
  - override /algo: ppo
  - override /env: gym
  - _self_

# Environment
env:
  capture_video: False
  num_envs: 1
  sync_env: True

# Algorithm
algo:
  name: ppo
  rollout_steps: 128
  normalize_advantages: True
  max_grad_norm: 0.5
  encoder:
    mlp_features_dim: null
  actor:
    mlp_layers: 0
  critic:
    mlp_layers: 0
  optimizer:
    lr: 1.5e-3
  total_steps: 65536
  per_rank_batch_size: 64
  mlp_keys:
    encoder: [state]

# Buffer
buffer:
  share_data: False
  size: ${algo.rollout_steps}
  memmap: False

fabric:
  devices: 1
  accelerator: cpu

checkpoint:
  every: 70000
  save_last: False

metric:
  log_every: 70000
  log_level: 0
  disable_timer: True