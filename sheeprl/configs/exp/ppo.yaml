# @package _global_

defaults:
  - override /algo: ppo
  - override /env: gym
  - _self_

# Experiment
total_steps: 65536
per_rank_batch_size: 64

# Buffer
buffer:
  share_data: False
  size: ${algo.rollout_steps}

model_manager:
  models: 
    agent:
      model_name: "${exp_name}"
      description: "PPO Agent in ${env.id} Environment"
      tags: {}

metric:
  aggregator:
    metrics:
      Loss/value_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/entropy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}

mlp_keys:
  encoder: [state]