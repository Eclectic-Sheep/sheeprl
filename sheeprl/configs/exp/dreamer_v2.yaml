# @package _global_

defaults:
  - override /algo: dreamer_v2
  - override /env: atari
  - _self_

# Experiment
total_steps: 5000000
per_rank_batch_size: 16
per_rank_sequence_length: 50

# Model Manager
model_manager:
  models: 
    world_model:
      model_name: "${exp_name}_world_model"
      description: "DreamerV2 World Model used in ${env.id} Environment"
      tags: {}
    actor:
      model_name: "${exp_name}_actor"
      description: "DreamerV2 Actor used in ${env.id} Environment"
      tags: {}
    critic:
      model_name: "${exp_name}_critic"
      description: "DreamerV2 Critic used in ${env.id} Environment"
      tags: {}
    target_critic:
      model_name: "${exp_name}_target_critic"
      description: "DreamerV2 Target Critic used in ${env.id} Environment"
      tags: {}

# Checkpoint
checkpoint:
  every: 100000

# Buffer
buffer:
  size: 5000000
  type: sequential
  checkpoint: False
  prioritize_ends: False

# Distribution
distribution:
  type: "auto"

metric:
  aggregator:
    metrics:
      Loss/world_model_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/value_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/observation_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/reward_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/state_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/continue_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/post_entropy: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/prior_entropy: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/kl: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Params/exploration_amount: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/world_model: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/actor: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/critic: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}

cnn_keys:
  encoder: [rgb]
  decoder: [rgb]