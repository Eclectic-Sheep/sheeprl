# @package _global_

defaults:
  - dreamer_v2
  - override /algo: p2e_dv2
  - _self_

total_steps: 1000000

algo:
  name: p2e_dv2_finetuning
  learning_starts: 10000
  player:
    actor_type: exploration

buffer:
  load_from_exploration: False

checkpoint:
  exploration_ckpt_path: ???

# Model Manager
model_manager:
  models: 
    world_model:
      model_name: "${exp_name}_world_model"
      description: "P2E_DV2 World Model used in ${env.id} Environment"
      tags: {}
    actor_task:
      model_name: "${exp_name}_actor_task"
      description: "P2E_DV2 Actor Task used in ${env.id} Environment"
      tags: {}
    critic_task:
      model_name: "${exp_name}_critic_task"
      description: "P2E_DV2 Critic Task used in ${env.id} Environment"
      tags: {}
    target_critic_task:
      model_name: "${exp_name}_target_critic_task"
      description: "P2E_DV2 Target Critic Task used in ${env.id} Environment"
      tags: {}

metric:
  aggregator:
    metrics:
      Loss/world_model_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/value_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/observation_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/reward_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/state_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/continue_loss: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/post_entropy: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/prior_entropy: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      State/kl: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Params/exploration_amount_task: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Params/exploration_amount_exploration: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/world_model: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/actor: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Grads/critic: 
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}