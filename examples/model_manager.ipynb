{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Manager\n",
    "\n",
    "In this notebook we prenset the [MlflowModelManager](../sheeprl/utils/model_manager.py) and a possible use.\n",
    "It includes methods such as:\n",
    "* Register the model\n",
    "* Retrieve latest version\n",
    "* Transition the model to a new stage\n",
    "* Delete the model\n",
    "\n",
    "First of all, we need to run Mlflow server with artifact store. You can find the instructions for running Mlflow server [here](https://mlflow.org/docs/latest/tracking.html#tracking-ui). Let's open a new terminal and run the following command:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> This is one of the possibilities, you could have the server running on another machine, so you just need to set the `tracking_uri` parameter properly.\n",
    "\n",
    "### Running the Experiment and Registering the Model\n",
    "Second, we launch an experiment, so we need to retrieve the configs and execute the `run_algorithm` function. We train a PPO agent in the CartPole-v1 environment for few steps (we do not want to reach the best performance, but we want to show how SheepRL interprets model management for reinforcement learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Experiment with name mlflow_example not found. Creating it.\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-11-27_11-39-45_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-11-27_11-39-45_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:233: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Test - Reward: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/27 11:40:02 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-10-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'sheeprl'}\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'mlflow_example'.\n",
      "2023/11/27 11:40:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example with version 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'mlflow_example'.\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from sheeprl.utils.utils import dotdict\n",
    "from sheeprl.cli import run_algorithm\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo algo.total_steps=1024 model_manager.disabled=False logger@metric.logger=mlflow checkpoint.every=1024 exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"algo.total_steps=1024\",\n",
    "            \"model_manager.disabled=False\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"checkpoint.every=1024\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True))\n",
    "run_algorithm(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Experiment Info\n",
    "\n",
    "The experiment is logged on MLFlow, and we can retrieve it and the just  with the following instructions. Moreover, given the experiment it is possible to retrieve all the runs with the `mlflow.search_runs()` function.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> You can check these information also from a browser, by entering the MLFlow address in a browser, e.g., `http://localhost:5000` if you are running mlflow locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: <Experiment: artifact_location='mlflow-artifacts:/824848876805206875', creation_time=1701081585725, experiment_id='824848876805206875', last_update_time=1701081585725, lifecycle_stage='active', name='mlflow_example', tags={}>\n",
      "Experiment (824848876805206875) runs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.Time/sps_env_interaction</th>\n",
       "      <th>metrics.Game/ep_len_avg</th>\n",
       "      <th>metrics.Info/clip_coef</th>\n",
       "      <th>metrics.Loss/value_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>params.metric/logger/prefix</th>\n",
       "      <th>params.algo/critic/mlp_layers</th>\n",
       "      <th>params.fabric/devices</th>\n",
       "      <th>params.seed</th>\n",
       "      <th>params.algo/ent_coef</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ea5ffc8b921d488e888f9d2d0d8325ea</td>\n",
       "      <td>824848876805206875</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>mlflow-artifacts:/824848876805206875/ea5ffc8b9...</td>\n",
       "      <td>2023-11-27 10:39:45.836000+00:00</td>\n",
       "      <td>2023-11-27 10:40:02.559000+00:00</td>\n",
       "      <td>280.225104</td>\n",
       "      <td>22.953489</td>\n",
       "      <td>0.2</td>\n",
       "      <td>36.166283</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ppo_CartPole-v1_2023-11-27_11-39-45</td>\n",
       "      <td>/home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...</td>\n",
       "      <td>[{\"run_id\": \"ea5ffc8b921d488e888f9d2d0d8325ea\"...</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>mmilesi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id       experiment_id    status  \\\n",
       "0  ea5ffc8b921d488e888f9d2d0d8325ea  824848876805206875  FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  mlflow-artifacts:/824848876805206875/ea5ffc8b9...   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2023-11-27 10:39:45.836000+00:00 2023-11-27 10:40:02.559000+00:00   \n",
       "\n",
       "   metrics.Time/sps_env_interaction  metrics.Game/ep_len_avg  \\\n",
       "0                        280.225104                22.953489   \n",
       "\n",
       "   metrics.Info/clip_coef  metrics.Loss/value_loss  ...  \\\n",
       "0                     0.2                36.166283  ...   \n",
       "\n",
       "   params.metric/logger/prefix  params.algo/critic/mlp_layers  \\\n",
       "0                                                           2   \n",
       "\n",
       "   params.fabric/devices  params.seed  params.algo/ent_coef  \\\n",
       "0                      1           42                   0.0   \n",
       "\n",
       "                   tags.mlflow.runName  \\\n",
       "0  ppo_CartPole-v1_2023-11-27_11-39-45   \n",
       "\n",
       "                             tags.mlflow.source.name  \\\n",
       "0  /home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...   \n",
       "\n",
       "                       tags.mlflow.log-model.history tags.mlflow.source.type  \\\n",
       "0  [{\"run_id\": \"ea5ffc8b921d488e888f9d2d0d8325ea\"...                   LOCAL   \n",
       "\n",
       "  tags.mlflow.user  \n",
       "0          mmilesi  \n",
       "\n",
       "[1 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(cfg.metric.logger.tracking_uri)\n",
    "exp = mlflow.get_experiment_by_name(\"mlflow_example\")\n",
    "print(\"Experiment:\", exp)\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "print(f\"Experiment ({exp.experiment_id}) runs:\")\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Model Info\n",
    "Since we set the `model_manager.disabled` to `False` the PPO Agent is registered in MLFLow, we can get its information with the following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 27/11/2023 11:40:02 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 1\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.utils.model_manager import MlflowModelManager\n",
    "from lightning import Fabric\n",
    "\n",
    "fabric = Fabric(devices=1, accelerator=cfg.fabric.accelerator, precision=cfg.fabric.precision)\n",
    "fabric.launch()\n",
    "model_manager = MlflowModelManager(fabric, cfg.model_manager.tracking_uri)\n",
    "\n",
    "model_info = mlflow.search_registered_models(filter_string=\"name='mlflow_example'\")[-1]\n",
    "model_name = model_info.name\n",
    "print(\"Name:\", model_name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a New Model Version from Checkpoint\n",
    "\n",
    "Suppose to train a new PPO Agent in the CartPole-v1 environment and to obtain better results than before. You can register a new version of the model. To do this, we show another method to register models, not directly after training, but from a checkpoint.\n",
    "\n",
    "First of all, we need to run another experiment with different hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-11-27_11-40-36_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-11-27_11-40-36_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:233: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n",
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Rank-0: policy_step=1056, reward_env_3=19.0\n",
      "Rank-0: policy_step=1068, reward_env_0=18.0\n",
      "Rank-0: policy_step=1080, reward_env_2=29.0\n",
      "Rank-0: policy_step=1112, reward_env_1=26.0\n",
      "Rank-0: policy_step=1124, reward_env_2=11.0\n",
      "Rank-0: policy_step=1136, reward_env_0=17.0\n",
      "Rank-0: policy_step=1152, reward_env_3=24.0\n",
      "Rank-0: policy_step=1212, reward_env_0=19.0\n",
      "Rank-0: policy_step=1240, reward_env_2=29.0\n",
      "Rank-0: policy_step=1284, reward_env_0=18.0\n",
      "Rank-0: policy_step=1284, reward_env_1=43.0\n",
      "Rank-0: policy_step=1320, reward_env_2=20.0\n",
      "Rank-0: policy_step=1336, reward_env_3=46.0\n",
      "Rank-0: policy_step=1364, reward_env_1=20.0\n",
      "Rank-0: policy_step=1384, reward_env_0=25.0\n",
      "Rank-0: policy_step=1408, reward_env_3=18.0\n",
      "Rank-0: policy_step=1432, reward_env_2=28.0\n",
      "Rank-0: policy_step=1448, reward_env_0=16.0\n",
      "Rank-0: policy_step=1472, reward_env_1=27.0\n",
      "Rank-0: policy_step=1480, reward_env_3=18.0\n",
      "Rank-0: policy_step=1500, reward_env_2=17.0\n",
      "Rank-0: policy_step=1516, reward_env_0=17.0\n",
      "Rank-0: policy_step=1580, reward_env_0=16.0\n",
      "Rank-0: policy_step=1644, reward_env_1=43.0\n",
      "Rank-0: policy_step=1664, reward_env_2=41.0\n",
      "Rank-0: policy_step=1668, reward_env_0=22.0\n",
      "Rank-0: policy_step=1692, reward_env_3=53.0\n",
      "Rank-0: policy_step=1744, reward_env_1=25.0\n",
      "Rank-0: policy_step=1768, reward_env_0=25.0\n",
      "Rank-0: policy_step=1796, reward_env_2=33.0\n",
      "Rank-0: policy_step=1864, reward_env_2=17.0\n",
      "Rank-0: policy_step=1956, reward_env_0=47.0\n",
      "Rank-0: policy_step=1960, reward_env_1=54.0\n",
      "Rank-0: policy_step=1964, reward_env_2=25.0\n",
      "Rank-0: policy_step=2012, reward_env_2=12.0\n",
      "Rank-0: policy_step=2040, reward_env_1=20.0\n",
      "Rank-0: policy_step=2152, reward_env_1=28.0\n",
      "Rank-0: policy_step=2164, reward_env_3=118.0\n",
      "Rank-0: policy_step=2196, reward_env_1=11.0\n",
      "Rank-0: policy_step=2260, reward_env_2=62.0\n",
      "Rank-0: policy_step=2268, reward_env_0=78.0\n",
      "Rank-0: policy_step=2276, reward_env_1=20.0\n",
      "Rank-0: policy_step=2308, reward_env_3=36.0\n",
      "Rank-0: policy_step=2492, reward_env_2=58.0\n",
      "Rank-0: policy_step=2520, reward_env_0=63.0\n",
      "Rank-0: policy_step=2584, reward_env_2=23.0\n",
      "Rank-0: policy_step=2608, reward_env_1=83.0\n",
      "Rank-0: policy_step=2632, reward_env_3=81.0\n",
      "Rank-0: policy_step=2692, reward_env_0=43.0\n",
      "Rank-0: policy_step=2780, reward_env_1=43.0\n",
      "Rank-0: policy_step=2796, reward_env_2=53.0\n",
      "Rank-0: policy_step=2928, reward_env_0=59.0\n",
      "Rank-0: policy_step=2940, reward_env_2=36.0\n",
      "Rank-0: policy_step=2944, reward_env_3=78.0\n",
      "Rank-0: policy_step=3012, reward_env_1=58.0\n",
      "Rank-0: policy_step=3080, reward_env_0=38.0\n",
      "Rank-0: policy_step=3224, reward_env_3=70.0\n",
      "Rank-0: policy_step=3284, reward_env_1=68.0\n",
      "Rank-0: policy_step=3284, reward_env_2=86.0\n",
      "Rank-0: policy_step=3288, reward_env_0=52.0\n",
      "Rank-0: policy_step=3480, reward_env_2=49.0\n",
      "Rank-0: policy_step=3548, reward_env_3=81.0\n",
      "Rank-0: policy_step=3588, reward_env_1=76.0\n",
      "Rank-0: policy_step=3712, reward_env_2=58.0\n",
      "Rank-0: policy_step=3840, reward_env_1=63.0\n",
      "Rank-0: policy_step=3928, reward_env_2=54.0\n",
      "Rank-0: policy_step=3956, reward_env_3=102.0\n",
      "Rank-0: policy_step=4016, reward_env_0=182.0\n",
      "Rank-0: policy_step=4096, reward_env_1=64.0\n",
      "Rank-0: policy_step=4252, reward_env_1=39.0\n",
      "Rank-0: policy_step=4256, reward_env_0=60.0\n",
      "Rank-0: policy_step=4316, reward_env_3=90.0\n",
      "Rank-0: policy_step=4416, reward_env_1=41.0\n",
      "Rank-0: policy_step=4476, reward_env_2=137.0\n",
      "Rank-0: policy_step=4776, reward_env_1=90.0\n",
      "Rank-0: policy_step=4800, reward_env_0=136.0\n",
      "Rank-0: policy_step=4836, reward_env_2=90.0\n",
      "Rank-0: policy_step=4920, reward_env_3=151.0\n",
      "Rank-0: policy_step=5176, reward_env_1=100.0\n",
      "Rank-0: policy_step=5308, reward_env_0=127.0\n",
      "Rank-0: policy_step=5332, reward_env_2=124.0\n",
      "Rank-0: policy_step=5388, reward_env_3=117.0\n",
      "Rank-0: policy_step=5556, reward_env_0=62.0\n",
      "Rank-0: policy_step=5736, reward_env_3=87.0\n",
      "Rank-0: policy_step=5744, reward_env_1=142.0\n",
      "Rank-0: policy_step=5832, reward_env_2=125.0\n",
      "Rank-0: policy_step=5968, reward_env_0=103.0\n",
      "Rank-0: policy_step=6168, reward_env_1=106.0\n",
      "Rank-0: policy_step=6272, reward_env_3=134.0\n",
      "Rank-0: policy_step=6340, reward_env_2=127.0\n",
      "Rank-0: policy_step=6408, reward_env_0=110.0\n",
      "Rank-0: policy_step=6756, reward_env_1=147.0\n",
      "Rank-0: policy_step=6856, reward_env_3=146.0\n",
      "Rank-0: policy_step=6876, reward_env_2=134.0\n",
      "Rank-0: policy_step=6920, reward_env_0=128.0\n",
      "Rank-0: policy_step=7308, reward_env_1=138.0\n",
      "Rank-0: policy_step=7644, reward_env_3=197.0\n",
      "Rank-0: policy_step=7688, reward_env_2=203.0\n",
      "Rank-0: policy_step=7700, reward_env_0=195.0\n",
      "Rank-0: policy_step=8080, reward_env_1=193.0\n",
      "Rank-0: policy_step=8636, reward_env_2=237.0\n",
      "Rank-0: policy_step=8644, reward_env_3=250.0\n",
      "Rank-0: policy_step=8756, reward_env_0=264.0\n",
      "Rank-0: policy_step=8868, reward_env_1=197.0\n",
      "Rank-0: policy_step=9728, reward_env_2=273.0\n",
      "Rank-0: policy_step=9864, reward_env_3=305.0\n",
      "Rank-0: policy_step=10756, reward_env_0=500.0\n",
      "Rank-0: policy_step=10848, reward_env_1=495.0\n",
      "Rank-0: policy_step=11124, reward_env_2=349.0\n",
      "Rank-0: policy_step=11472, reward_env_3=402.0\n",
      "Rank-0: policy_step=12756, reward_env_0=500.0\n",
      "Rank-0: policy_step=12848, reward_env_1=500.0\n",
      "Rank-0: policy_step=13124, reward_env_2=500.0\n",
      "Rank-0: policy_step=13472, reward_env_3=500.0\n",
      "Rank-0: policy_step=14324, reward_env_1=369.0\n",
      "Rank-0: policy_step=14532, reward_env_0=444.0\n",
      "Rank-0: policy_step=14796, reward_env_3=331.0\n",
      "Rank-0: policy_step=15112, reward_env_2=497.0\n",
      "Rank-0: policy_step=15924, reward_env_1=400.0\n",
      "Rank-0: policy_step=16000, reward_env_0=367.0\n",
      "Rank-0: policy_step=16044, reward_env_2=233.0\n",
      "Rank-0: policy_step=16152, reward_env_3=339.0\n",
      "Test - Reward: 392.0\n"
     ]
    }
   ],
   "source": [
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo algo.total_steps=16384 checkpoint.every=16384 logger@metric.logger=mlflow exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "import os\n",
    "\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg_ = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"algo.total_steps=16384\",\n",
    "            \"checkpoint.every=16384\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg_, resolve=True, throw_on_missing=True))\n",
    "run_algorithm(cfg)\n",
    "os.mkdir(f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/\")\n",
    "OmegaConf.save(cfg_, f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `./sheeprl_model_manager.py` script to take a checkpoint and register the models of the checkpoint.\n",
    "We want to retrieve the id of the last run, to associate the model to the correct run. We can take it from the UI (from the browser) or by retrieving it with the `mlflow.search_runs(experiment_ids=[exp.experiment_id])` instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/27 11:44:51 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-10-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'sheeprl'}\n",
      "Registered model 'mlflow_example' already exists. Creating a new version of this model...\n",
      "2023/11/27 11:44:51 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example with version 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'mlflow_example'.\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.cli import registration\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl_model_manager.py checkpoint_path=</path/to/checkpoint.ckpt> \\\n",
    "# model_manager=ppo model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment' \\\n",
    "# run.id=<run_id>`\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "run_id = runs[\"run_id\"][0]\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"model_manager_config.yaml\",\n",
    "        overrides=[\n",
    "            # Substitute the checkpoint path with your /path/to/checkpoint.ckpt\n",
    "            \"checkpoint_path=./logs/runs/ppo/CartPole-v1/2023-11-27_11-40-36_mlflow_example_42/version_0/checkpoint/ckpt_16384_0.ckpt\",\n",
    "            \"model_manager=ppo\",\n",
    "            \"model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment'\",\n",
    "            f\"run.id={run_id}\",\n",
    "        ],\n",
    "    )\n",
    "registration(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, we can retrieve the new information of the registred model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 27/11/2023 11:40:02 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "## **Version 2**\n",
      "### Author: mmilesi\n",
      "### Date: 27/11/2023 11:44:51 CET\n",
      "### Description: \n",
      "New PPO Agent version trained in CartPole-v1 environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 2\n"
     ]
    }
   ],
   "source": [
    "model_info = mlflow.search_registered_models(filter_string=f\"name='{model_name}'\")[-1]\n",
    "print(\"Name:\", model_info.name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging the Model\n",
    "After registering the model, we can transition the model to a new stage. We can transition the model to staging stage with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitioning model mlflow_example version 2 from None to staging\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1701081891396, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 27/11/2023 11:44:51 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'), last_updated_timestamp=1701081991402, name='mlflow_example', run_id='fa2f6f3e9c1546a68cd641fa5b5170b6', run_link='', source='mlflow-artifacts:/824848876805206875/fa2f6f3e9c1546a68cd641fa5b5170b6/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.transition_model(\n",
    "    model_name=\"mlflow_example\", version=latest_version.version, stage=\"staging\", description=\"Staging Model for demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Model\n",
    "You can download the registered models and load with the `torch.load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model mlflow_example version 2 from mlflow-artifacts:/824848876805206875/fa2f6f3e9c1546a68cd641fa5b5170b6/artifacts/agent to ./models/ppo-agent-cartpole\n",
      "Creating output path ./models/ppo-agent-cartpole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 23.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOAgent(\n",
       "  (feature_extractor): MultiEncoder(\n",
       "    (mlp_encoder): MLPEncoder(\n",
       "      (model): MLP(\n",
       "        (_model): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "          (1): Tanh()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Tanh()\n",
       "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (critic): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (actor_backbone): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (actor_heads): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "download_path = \"./models/ppo-agent-cartpole\"\n",
    "model_manager.download_model(model_name, latest_version.version, download_path)\n",
    "agent = torch.load(\"models/ppo-agent-cartpole/agent/data/model.pth\")\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Best Models\n",
    "Another possibility is to register the best models of a specific experiment. Let us suppose we want to register the best model of the two experiments we run before: the only thing we have to do is to call the `model_manager.register_best_models()` function by specifing the `experiment_name`, the `metric`, and the `models_info` (a python dictionary containing the name, the path, the desctription and the tags of the models we want to register), as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ppo_agent_best_reward_cartpole'.\n",
      "2023/11/27 11:47:14 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ppo_agent_best_reward_cartpole, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model ppo_agent_best_reward_cartpole with version 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'ppo_agent_best_reward_cartpole'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agent': <ModelVersion: aliases=[], creation_timestamp=1701082034845, current_stage='None', description='', last_updated_timestamp=1701082034845, name='ppo_agent_best_reward_cartpole', run_id='fa2f6f3e9c1546a68cd641fa5b5170b6', run_link='', source='mlflow-artifacts:/824848876805206875/fa2f6f3e9c1546a68cd641fa5b5170b6/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='1'>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_info = dotdict(\n",
    "    {\n",
    "        \"agent\": {\n",
    "            \"name\": \"ppo_agent_cartpole_best_reward\",\n",
    "            \"path\": \"agent\",\n",
    "            \"tags\": {},\n",
    "            \"description\": \"The best PPO Agent in CartPole environment.\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "model_manager.register_best_models(\"mlflow_example\", models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Model\n",
    "Finally, you can delete registered models you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model mlflow_example version 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RegisteredModel: aliases={}, creation_timestamp=1701081602575, description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 1**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 27/11/2023 11:40:02 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'PPO Agent in CartPole-v1 Environment\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 27/11/2023 11:44:51 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 27/11/2023 11:46:31 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'\n",
       " '## **Deletion:**\\n'\n",
       " '### Version 1 from stage: None\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 27/11/2023 11:48:01 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Delete model version 1\\n'), last_updated_timestamp=1701082081894, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1701081891396, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 27/11/2023 11:44:51 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 27/11/2023 11:46:31 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'), last_updated_timestamp=1701081991453, name='mlflow_example', run_id='fa2f6f3e9c1546a68cd641fa5b5170b6', run_link='', source='mlflow-artifacts:/824848876805206875/fa2f6f3e9c1546a68cd641fa5b5170b6/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>], name='mlflow_example', tags={}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.delete_model(\n",
    "    model_name, int(latest_version.version) - 1, f\"Delete model version {int(latest_version.version)-1}\"\n",
    ")\n",
    "mlflow.search_registered_models(filter_string=\"name='mlflow_example'\")[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
