{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> This notebook was inspired by [https://orobix.github.io/quadra/v1.3.6/tutorials/model_management.html](https://orobix.github.io/quadra/v1.3.6/tutorials/model_management.html)\n",
    "\n",
    "# Model Manager\n",
    "\n",
    "In this notebook, we present the [MlflowModelManager](../sheeprl/utils/model_manager.py) and possible use.\n",
    "It includes methods such as:\n",
    "* Register the model\n",
    "* Retrieve the latest version\n",
    "* Transition the model to a new stage\n",
    "* Delete the model\n",
    "\n",
    "First of all, we need to run the Mlflow server with the artifact store. You can find the instructions for running the Mlflow server [here](https://mlflow.org/docs/latest/tracking.html#tracking-ui). Let's open a new terminal and run the following command:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> This is one of the possibilities, you could have the server running on another machine, so you just need to set the `tracking_uri` parameter properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment and Registering the Model\n",
    "Second, we launch an experiment, so we need to retrieve the configs and execute the `run_algorithm` function. We train a PPO agent in the CartPole-v1 environment for few steps (we do not want to reach the best performance, but we want to show how SheepRL interprets model management for reinforcement learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Experiment with name mlflow_example not found. Creating it.\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-11-28_11-50-38_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-11-28_11-50-38_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:233: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n",
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Test - Reward: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/28 11:50:46 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-10-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'sheeprl'}\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'mlflow_example'.\n",
      "2023/11/28 11:50:46 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example with version 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'mlflow_example'.\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from sheeprl.utils.utils import dotdict\n",
    "from sheeprl.cli import check_configs, run_algorithm\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo algo.total_steps=1024 model_manager.disabled=False logger@metric.logger=mlflow checkpoint.every=1024 exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"algo.total_steps=1024\",\n",
    "            \"model_manager.disabled=False\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"checkpoint.every=1024\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True))\n",
    "check_configs(cfg)\n",
    "run_algorithm(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Experiment Info\n",
    "\n",
    "The experiment is logged on MLFlow, and we can retrieve it just  with the following instructions. Moreover, given the experiment, it is possible to retrieve all the runs with the `mlflow.search_runs()` function.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> You can check this information from a browser, by entering the MLFlow address in a browser, e.g., `http://localhost:5000` if you are running mlflow locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: <Experiment: artifact_location='mlflow-artifacts:/257965132461445889', creation_time=1701168639065, experiment_id='257965132461445889', last_update_time=1701168639065, lifecycle_stage='active', name='mlflow_example', tags={}>\n",
      "Experiment (257965132461445889) runs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.Loss/value_loss</th>\n",
       "      <th>metrics.Test/cumulative_reward</th>\n",
       "      <th>metrics.Loss/entropy_loss</th>\n",
       "      <th>metrics.Rewards/rew_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>params.num_threads</th>\n",
       "      <th>params.algo/mlp_keys/encoder</th>\n",
       "      <th>params.metric/logger/experiment_name</th>\n",
       "      <th>params.algo/loss_reduction</th>\n",
       "      <th>params.algo/critic/dense_act</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7432e9e1e0a2491e9cf2b80c7cefe6c7</td>\n",
       "      <td>257965132461445889</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>mlflow-artifacts:/257965132461445889/7432e9e1e...</td>\n",
       "      <td>2023-11-28 10:50:39.184000+00:00</td>\n",
       "      <td>2023-11-28 10:50:46.452000+00:00</td>\n",
       "      <td>36.166283</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-0.687031</td>\n",
       "      <td>22.953489</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>['state']</td>\n",
       "      <td>mlflow_example</td>\n",
       "      <td>mean</td>\n",
       "      <td>torch.nn.Tanh</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>mmilesi</td>\n",
       "      <td>/home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...</td>\n",
       "      <td>[{\"run_id\": \"7432e9e1e0a2491e9cf2b80c7cefe6c7\"...</td>\n",
       "      <td>ppo_CartPole-v1_2023-11-28_11-50-38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id       experiment_id    status  \\\n",
       "0  7432e9e1e0a2491e9cf2b80c7cefe6c7  257965132461445889  FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  mlflow-artifacts:/257965132461445889/7432e9e1e...   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2023-11-28 10:50:39.184000+00:00 2023-11-28 10:50:46.452000+00:00   \n",
       "\n",
       "   metrics.Loss/value_loss  metrics.Test/cumulative_reward  \\\n",
       "0                36.166283                            48.0   \n",
       "\n",
       "   metrics.Loss/entropy_loss  metrics.Rewards/rew_avg  ...  \\\n",
       "0                  -0.687031                22.953489  ...   \n",
       "\n",
       "   params.num_threads  params.algo/mlp_keys/encoder  \\\n",
       "0                   1                     ['state']   \n",
       "\n",
       "   params.metric/logger/experiment_name  params.algo/loss_reduction  \\\n",
       "0                        mlflow_example                        mean   \n",
       "\n",
       "   params.algo/critic/dense_act  tags.mlflow.source.type  tags.mlflow.user  \\\n",
       "0                 torch.nn.Tanh                    LOCAL           mmilesi   \n",
       "\n",
       "                             tags.mlflow.source.name  \\\n",
       "0  /home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...   \n",
       "\n",
       "                       tags.mlflow.log-model.history  \\\n",
       "0  [{\"run_id\": \"7432e9e1e0a2491e9cf2b80c7cefe6c7\"...   \n",
       "\n",
       "                   tags.mlflow.runName  \n",
       "0  ppo_CartPole-v1_2023-11-28_11-50-38  \n",
       "\n",
       "[1 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(cfg.metric.logger.tracking_uri)\n",
    "exp = mlflow.get_experiment_by_name(\"mlflow_example\")\n",
    "print(\"Experiment:\", exp)\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "print(f\"Experiment ({exp.experiment_id}) runs:\")\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Model Info\n",
    "Since we set the `model_manager.disabled` to `False` the PPO Agent is registered in MLFLow, we can get its information with the following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 28/11/2023 11:50:46 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 1\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.utils.model_manager import MlflowModelManager\n",
    "from lightning import Fabric\n",
    "\n",
    "fabric = Fabric(devices=1, accelerator=cfg.fabric.accelerator, precision=cfg.fabric.precision)\n",
    "fabric.launch()\n",
    "model_manager = MlflowModelManager(fabric, cfg.model_manager.tracking_uri)\n",
    "\n",
    "model_info = mlflow.search_registered_models(filter_string=\"name='mlflow_example'\")[-1]\n",
    "model_name = model_info.name\n",
    "print(\"Name:\", model_name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a New Model Version from Checkpoint\n",
    "\n",
    "Suppose to train a new PPO Agent in the CartPole-v1 environment and to obtain better results than before. You can register a new version of the model. To do this, we show another method to register models, not directly after training, but from a checkpoint.\n",
    "\n",
    "First of all, we need to run another experiment with different hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-11-28_11-50-46_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-11-28_11-50-46_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:233: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n",
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Rank-0: policy_step=1056, reward_env_3=19.0\n",
      "Rank-0: policy_step=1068, reward_env_0=18.0\n",
      "Rank-0: policy_step=1080, reward_env_2=29.0\n",
      "Rank-0: policy_step=1112, reward_env_1=26.0\n",
      "Rank-0: policy_step=1124, reward_env_2=11.0\n",
      "Rank-0: policy_step=1136, reward_env_0=17.0\n",
      "Rank-0: policy_step=1152, reward_env_3=24.0\n",
      "Rank-0: policy_step=1212, reward_env_0=19.0\n",
      "Rank-0: policy_step=1240, reward_env_2=29.0\n",
      "Rank-0: policy_step=1284, reward_env_0=18.0\n",
      "Rank-0: policy_step=1284, reward_env_1=43.0\n",
      "Rank-0: policy_step=1320, reward_env_2=20.0\n",
      "Rank-0: policy_step=1336, reward_env_3=46.0\n",
      "Rank-0: policy_step=1364, reward_env_1=20.0\n",
      "Rank-0: policy_step=1384, reward_env_0=25.0\n",
      "Rank-0: policy_step=1408, reward_env_3=18.0\n",
      "Rank-0: policy_step=1432, reward_env_2=28.0\n",
      "Rank-0: policy_step=1448, reward_env_0=16.0\n",
      "Rank-0: policy_step=1472, reward_env_1=27.0\n",
      "Rank-0: policy_step=1480, reward_env_3=18.0\n",
      "Rank-0: policy_step=1500, reward_env_2=17.0\n",
      "Rank-0: policy_step=1516, reward_env_0=17.0\n",
      "Rank-0: policy_step=1580, reward_env_0=16.0\n",
      "Rank-0: policy_step=1644, reward_env_1=43.0\n",
      "Rank-0: policy_step=1664, reward_env_2=41.0\n",
      "Rank-0: policy_step=1668, reward_env_0=22.0\n",
      "Rank-0: policy_step=1692, reward_env_3=53.0\n",
      "Rank-0: policy_step=1744, reward_env_1=25.0\n",
      "Rank-0: policy_step=1768, reward_env_0=25.0\n",
      "Rank-0: policy_step=1796, reward_env_2=33.0\n",
      "Rank-0: policy_step=1864, reward_env_2=17.0\n",
      "Rank-0: policy_step=1956, reward_env_0=47.0\n",
      "Rank-0: policy_step=1960, reward_env_1=54.0\n",
      "Rank-0: policy_step=1964, reward_env_2=25.0\n",
      "Rank-0: policy_step=2012, reward_env_2=12.0\n",
      "Rank-0: policy_step=2040, reward_env_1=20.0\n",
      "Rank-0: policy_step=2152, reward_env_1=28.0\n",
      "Rank-0: policy_step=2164, reward_env_3=118.0\n",
      "Rank-0: policy_step=2196, reward_env_1=11.0\n",
      "Rank-0: policy_step=2260, reward_env_2=62.0\n",
      "Rank-0: policy_step=2268, reward_env_0=78.0\n",
      "Rank-0: policy_step=2276, reward_env_1=20.0\n",
      "Rank-0: policy_step=2308, reward_env_3=36.0\n",
      "Rank-0: policy_step=2492, reward_env_2=58.0\n",
      "Rank-0: policy_step=2520, reward_env_0=63.0\n",
      "Rank-0: policy_step=2584, reward_env_2=23.0\n",
      "Rank-0: policy_step=2608, reward_env_1=83.0\n",
      "Rank-0: policy_step=2632, reward_env_3=81.0\n",
      "Rank-0: policy_step=2692, reward_env_0=43.0\n",
      "Rank-0: policy_step=2780, reward_env_1=43.0\n",
      "Rank-0: policy_step=2796, reward_env_2=53.0\n",
      "Rank-0: policy_step=2928, reward_env_0=59.0\n",
      "Rank-0: policy_step=2940, reward_env_2=36.0\n",
      "Rank-0: policy_step=2944, reward_env_3=78.0\n",
      "Rank-0: policy_step=3012, reward_env_1=58.0\n",
      "Rank-0: policy_step=3080, reward_env_0=38.0\n",
      "Rank-0: policy_step=3224, reward_env_3=70.0\n",
      "Rank-0: policy_step=3284, reward_env_1=68.0\n",
      "Rank-0: policy_step=3284, reward_env_2=86.0\n",
      "Rank-0: policy_step=3288, reward_env_0=52.0\n",
      "Rank-0: policy_step=3480, reward_env_2=49.0\n",
      "Rank-0: policy_step=3548, reward_env_3=81.0\n",
      "Rank-0: policy_step=3588, reward_env_1=76.0\n",
      "Rank-0: policy_step=3712, reward_env_2=58.0\n",
      "Rank-0: policy_step=3840, reward_env_1=63.0\n",
      "Rank-0: policy_step=3928, reward_env_2=54.0\n",
      "Rank-0: policy_step=3956, reward_env_3=102.0\n",
      "Rank-0: policy_step=4016, reward_env_0=182.0\n",
      "Rank-0: policy_step=4096, reward_env_1=64.0\n",
      "Rank-0: policy_step=4252, reward_env_1=39.0\n",
      "Rank-0: policy_step=4256, reward_env_0=60.0\n",
      "Rank-0: policy_step=4316, reward_env_3=90.0\n",
      "Rank-0: policy_step=4416, reward_env_1=41.0\n",
      "Rank-0: policy_step=4476, reward_env_2=137.0\n",
      "Rank-0: policy_step=4776, reward_env_1=90.0\n",
      "Rank-0: policy_step=4800, reward_env_0=136.0\n",
      "Rank-0: policy_step=4836, reward_env_2=90.0\n",
      "Rank-0: policy_step=4920, reward_env_3=151.0\n",
      "Rank-0: policy_step=5176, reward_env_1=100.0\n",
      "Rank-0: policy_step=5308, reward_env_0=127.0\n",
      "Rank-0: policy_step=5332, reward_env_2=124.0\n",
      "Rank-0: policy_step=5388, reward_env_3=117.0\n",
      "Rank-0: policy_step=5556, reward_env_0=62.0\n",
      "Rank-0: policy_step=5736, reward_env_3=87.0\n",
      "Rank-0: policy_step=5744, reward_env_1=142.0\n",
      "Rank-0: policy_step=5832, reward_env_2=125.0\n",
      "Rank-0: policy_step=5968, reward_env_0=103.0\n",
      "Rank-0: policy_step=6168, reward_env_1=106.0\n",
      "Rank-0: policy_step=6272, reward_env_3=134.0\n",
      "Rank-0: policy_step=6340, reward_env_2=127.0\n",
      "Rank-0: policy_step=6408, reward_env_0=110.0\n",
      "Rank-0: policy_step=6756, reward_env_1=147.0\n",
      "Rank-0: policy_step=6856, reward_env_3=146.0\n",
      "Rank-0: policy_step=6876, reward_env_2=134.0\n",
      "Rank-0: policy_step=6920, reward_env_0=128.0\n",
      "Rank-0: policy_step=7308, reward_env_1=138.0\n",
      "Rank-0: policy_step=7644, reward_env_3=197.0\n",
      "Rank-0: policy_step=7688, reward_env_2=203.0\n",
      "Rank-0: policy_step=7700, reward_env_0=195.0\n",
      "Rank-0: policy_step=8080, reward_env_1=193.0\n",
      "Rank-0: policy_step=8636, reward_env_2=237.0\n",
      "Rank-0: policy_step=8644, reward_env_3=250.0\n",
      "Rank-0: policy_step=8756, reward_env_0=264.0\n",
      "Rank-0: policy_step=8868, reward_env_1=197.0\n",
      "Rank-0: policy_step=9728, reward_env_2=273.0\n",
      "Rank-0: policy_step=9864, reward_env_3=305.0\n",
      "Rank-0: policy_step=10756, reward_env_0=500.0\n",
      "Rank-0: policy_step=10848, reward_env_1=495.0\n",
      "Rank-0: policy_step=11124, reward_env_2=349.0\n",
      "Rank-0: policy_step=11472, reward_env_3=402.0\n",
      "Rank-0: policy_step=12756, reward_env_0=500.0\n",
      "Rank-0: policy_step=12848, reward_env_1=500.0\n",
      "Rank-0: policy_step=13124, reward_env_2=500.0\n",
      "Rank-0: policy_step=13472, reward_env_3=500.0\n",
      "Rank-0: policy_step=14324, reward_env_1=369.0\n",
      "Rank-0: policy_step=14532, reward_env_0=444.0\n",
      "Rank-0: policy_step=14796, reward_env_3=331.0\n",
      "Rank-0: policy_step=15112, reward_env_2=497.0\n",
      "Rank-0: policy_step=15924, reward_env_1=400.0\n",
      "Rank-0: policy_step=16000, reward_env_0=367.0\n",
      "Rank-0: policy_step=16044, reward_env_2=233.0\n",
      "Rank-0: policy_step=16152, reward_env_3=339.0\n",
      "Test - Reward: 392.0\n"
     ]
    }
   ],
   "source": [
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo algo.total_steps=16384 checkpoint.every=16384 logger@metric.logger=mlflow exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "import os\n",
    "\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg_ = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"algo.total_steps=16384\",\n",
    "            \"checkpoint.every=16384\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg_, resolve=True, throw_on_missing=True))\n",
    "run_algorithm(cfg)\n",
    "os.mkdir(f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/\")\n",
    "OmegaConf.save(cfg_, f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `./sheeprl_model_manager.py` script to take a checkpoint and register the models of the checkpoint.\n",
    "We want to retrieve the id of the last run, to associate the model to the correct run. We can take it from the UI (from the browser) or by retrieving it with the `mlflow.search_runs(experiment_ids=[exp.experiment_id])` instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/28 11:51:55 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-10-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'sheeprl'}\n",
      "Registered model 'mlflow_example' already exists. Creating a new version of this model...\n",
      "2023/11/28 11:51:55 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example with version 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'mlflow_example'.\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.cli import registration\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl_model_manager.py checkpoint_path=</path/to/checkpoint.ckpt> \\\n",
    "# model_manager=ppo model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment' \\\n",
    "# run.id=<run_id>`\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "run_id = runs[\"run_id\"][0]\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"model_manager_config.yaml\",\n",
    "        overrides=[\n",
    "            # Substitute the checkpoint path with your /path/to/checkpoint.ckpt\n",
    "            \"checkpoint_path=./logs/runs/ppo/CartPole-v1/2023-11-28_11-50-46_mlflow_example_42/version_0/checkpoint/ckpt_16384_0.ckpt\",\n",
    "            \"model_manager=ppo\",\n",
    "            \"model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment'\",\n",
    "            f\"run.id={run_id}\",\n",
    "        ],\n",
    "    )\n",
    "registration(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, we can retrieve the new information of the registered model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 28/11/2023 11:50:46 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "## **Version 2**\n",
      "### Author: mmilesi\n",
      "### Date: 28/11/2023 11:51:55 CET\n",
      "### Description: \n",
      "New PPO Agent version trained in CartPole-v1 environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 2\n"
     ]
    }
   ],
   "source": [
    "model_info = mlflow.search_registered_models(filter_string=f\"name='{model_name}'\")[-1]\n",
    "print(\"Name:\", model_info.name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging the Model\n",
    "After registering the model, we can transition the model to a new stage. We can transition the model to the `\"staging\"` stage with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitioning model mlflow_example version 2 from None to staging\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1701168715202, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 28/11/2023 11:51:55 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'), last_updated_timestamp=1701168721759, name='mlflow_example', run_id='7b8fcd3b2615483a9546380cf8f313c4', run_link='', source='mlflow-artifacts:/257965132461445889/7b8fcd3b2615483a9546380cf8f313c4/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.transition_model(\n",
    "    model_name=\"mlflow_example\", version=latest_version.version, stage=\"staging\", description=\"Staging Model for demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Model\n",
    "You can download the registered models and load them with the `torch.load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model mlflow_example version 2 from mlflow-artifacts:/257965132461445889/7b8fcd3b2615483a9546380cf8f313c4/artifacts/agent to ./models/ppo-agent-cartpole\n",
      "Creating output path ./models/ppo-agent-cartpole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 218.04it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOAgent(\n",
       "  (feature_extractor): MultiEncoder(\n",
       "    (mlp_encoder): MLPEncoder(\n",
       "      (model): MLP(\n",
       "        (_model): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "          (1): Tanh()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Tanh()\n",
       "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (critic): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (actor_backbone): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (actor_heads): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "download_path = \"./models/ppo-agent-cartpole\"\n",
    "model_manager.download_model(model_name, latest_version.version, download_path)\n",
    "agent = torch.load(\"models/ppo-agent-cartpole/agent/data/model.pth\")\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Best Models\n",
    "Another possibility is to register the best models of a specific experiment. Let us suppose we want to register the best model of the two experiments we ran before: the only thing we have to do is to call the `model_manager.register_best_models()` function by specifying the `experiment_name`, the `metric`, and the `models_info` (a python dictionary containing the name, the path, the description and the tags of the models we want to register), as shown below.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> If your experiment contains different agents, and each agent has different model paths, then you have to specify in the `models_info` all the models you want to register (i.e., the union of the models of all the agents). The MLFlow model manager will automatically select the correct models for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ppo_agent_cartpole_best_reward'.\n",
      "2023/11/28 11:52:11 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ppo_agent_cartpole_best_reward, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model ppo_agent_cartpole_best_reward with version 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'ppo_agent_cartpole_best_reward'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agent': <ModelVersion: aliases=[], creation_timestamp=1701168731379, current_stage='None', description='', last_updated_timestamp=1701168731379, name='ppo_agent_cartpole_best_reward', run_id='7b8fcd3b2615483a9546380cf8f313c4', run_link='', source='mlflow-artifacts:/257965132461445889/7b8fcd3b2615483a9546380cf8f313c4/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='1'>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_info = {\n",
    "    \"agent\": {\n",
    "        \"name\": \"ppo_agent_cartpole_best_reward\",\n",
    "        \"path\": \"agent\",\n",
    "        \"tags\": {},\n",
    "        \"description\": \"The best PPO Agent in CartPole environment.\",\n",
    "    }\n",
    "}\n",
    "model_manager.register_best_models(\"mlflow_example\", models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Model\n",
    "Finally, you can delete registered models you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model mlflow_example version 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RegisteredModel: aliases={}, creation_timestamp=1701168646465, description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 1**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 28/11/2023 11:50:46 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'PPO Agent in CartPole-v1 Environment\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 28/11/2023 11:51:55 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 28/11/2023 11:52:01 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'\n",
       " '## **Deletion:**\\n'\n",
       " '### Version 1 from stage: None\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 28/11/2023 11:52:20 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Delete model version 1\\n'), last_updated_timestamp=1701168740007, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1701168715202, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 28/11/2023 11:51:55 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 28/11/2023 11:52:01 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'), last_updated_timestamp=1701168721787, name='mlflow_example', run_id='7b8fcd3b2615483a9546380cf8f313c4', run_link='', source='mlflow-artifacts:/257965132461445889/7b8fcd3b2615483a9546380cf8f313c4/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>], name='mlflow_example', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.delete_model(\n",
    "    model_name, int(latest_version.version) - 1, f\"Delete model version {int(latest_version.version)-1}\"\n",
    ")\n",
    "mlflow.search_registered_models(filter_string=\"name='mlflow_example'\")[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
