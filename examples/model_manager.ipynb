{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> This notebook was inspired by [https://orobix.github.io/quadra/v1.3.6/tutorials/model_management.html](https://orobix.github.io/quadra/v1.3.6/tutorials/model_management.html)\n",
    "\n",
    "# Model Manager\n",
    "\n",
    "In this notebook, we present the [MlflowModelManager](../sheeprl/utils/model_manager.py) and possible use.\n",
    "It includes methods such as:\n",
    "* Register the model\n",
    "* Retrieve the latest version\n",
    "* Transition the model to a new stage\n",
    "* Delete the model\n",
    "\n",
    "First of all, we need to run the Mlflow server with the artifact store. You can find the instructions for running the Mlflow server [here](https://mlflow.org/docs/latest/tracking.html#tracking-ui). Let's open a new terminal and run the following command:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> This is one of the possibilities, you could have the server running on another machine, so you just need to set the `tracking_uri` parameter properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment and Registering the Model\n",
    "Second, we launch an experiment, so we need to retrieve the configs and execute the `run_algorithm` function. We train a PPO agent in the CartPole-v1 environment for few steps (we do not want to reach the best performance, but we want to show how SheepRL interprets model management for reinforcement learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Experiment with name mlflow_example not found. Creating it.\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-12-07_12-45-58_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-12-07_12-45-58_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:240: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n",
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Test - Reward: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/07 12:46:10 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpvcs2npwv/model/data, flavor: pytorch), fall back to return ['torch==2.0.1', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'mlflow_example_agent'.\n",
      "2023/12/07 12:46:10 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example_agent, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example_agent with version 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'mlflow_example_agent'.\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from sheeprl.utils.utils import dotdict\n",
    "from sheeprl.cli import check_configs, run_algorithm\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo algo.total_steps=1024 model_manager.disabled=False logger@metric.logger=mlflow checkpoint.every=1024 exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"algo.total_steps=1024\",\n",
    "            \"model_manager.disabled=False\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"checkpoint.every=1024\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True))\n",
    "check_configs(cfg)\n",
    "run_algorithm(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Experiment Info\n",
    "\n",
    "The experiment is logged on MLFlow, and we can retrieve it just  with the following instructions. Moreover, given the experiment, it is possible to retrieve all the runs with the `mlflow.search_runs()` function.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> You can check this information from a browser, by entering the MLFlow address in a browser, e.g., `http://localhost:5000` if you are running mlflow locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: <Experiment: artifact_location='mlflow-artifacts:/242317125620601262', creation_time=1701949559261, experiment_id='242317125620601262', last_update_time=1701949559261, lifecycle_stage='active', name='mlflow_example', tags={}>\n",
      "Experiment (242317125620601262) runs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.Loss/entropy_loss</th>\n",
       "      <th>metrics.Test/cumulative_reward</th>\n",
       "      <th>metrics.Info/ent_coef</th>\n",
       "      <th>metrics.Info/learning_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>params.algo/gae_lambda</th>\n",
       "      <th>params.env/action_repeat</th>\n",
       "      <th>params.env/grayscale</th>\n",
       "      <th>params.metric/aggregator/metrics/Loss/policy_loss/sync_on_compute</th>\n",
       "      <th>params.metric/log_level</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e453cf2114d43f28410803df985598a</td>\n",
       "      <td>242317125620601262</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>mlflow-artifacts:/242317125620601262/1e453cf21...</td>\n",
       "      <td>2023-12-07 11:45:59.641000+00:00</td>\n",
       "      <td>2023-12-07 11:46:10.350000+00:00</td>\n",
       "      <td>-0.687031</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>mmilesi</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>ppo_CartPole-v1_2023-12-07_12-45-58</td>\n",
       "      <td>/home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...</td>\n",
       "      <td>[{\"run_id\": \"1e453cf2114d43f28410803df985598a\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id       experiment_id    status  \\\n",
       "0  1e453cf2114d43f28410803df985598a  242317125620601262  FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  mlflow-artifacts:/242317125620601262/1e453cf21...   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2023-12-07 11:45:59.641000+00:00 2023-12-07 11:46:10.350000+00:00   \n",
       "\n",
       "   metrics.Loss/entropy_loss  metrics.Test/cumulative_reward  \\\n",
       "0                  -0.687031                            48.0   \n",
       "\n",
       "   metrics.Info/ent_coef  metrics.Info/learning_rate  ...  \\\n",
       "0                    0.0                       0.001  ...   \n",
       "\n",
       "   params.algo/gae_lambda  params.env/action_repeat  params.env/grayscale  \\\n",
       "0                    0.95                         1                 False   \n",
       "\n",
       "   params.metric/aggregator/metrics/Loss/policy_loss/sync_on_compute  \\\n",
       "0                                              False                   \n",
       "\n",
       "   params.metric/log_level  tags.mlflow.user  tags.mlflow.source.type  \\\n",
       "0                        1           mmilesi                    LOCAL   \n",
       "\n",
       "                   tags.mlflow.runName  \\\n",
       "0  ppo_CartPole-v1_2023-12-07_12-45-58   \n",
       "\n",
       "                             tags.mlflow.source.name  \\\n",
       "0  /home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...   \n",
       "\n",
       "                       tags.mlflow.log-model.history  \n",
       "0  [{\"run_id\": \"1e453cf2114d43f28410803df985598a\"...  \n",
       "\n",
       "[1 rows x 130 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(cfg.metric.logger.tracking_uri)\n",
    "exp = mlflow.get_experiment_by_name(\"mlflow_example\")\n",
    "print(\"Experiment:\", exp)\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "print(f\"Experiment ({exp.experiment_id}) runs:\")\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Model Info\n",
    "Since we set the `model_manager.disabled` to `False` the PPO Agent is registered in MLFLow, we can get its information with the following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example_agent\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 07/12/2023 12:46:10 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 1\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.utils.mlflow import MlflowModelManager\n",
    "from lightning import Fabric\n",
    "\n",
    "fabric = Fabric(devices=1, accelerator=cfg.fabric.accelerator, precision=cfg.fabric.precision)\n",
    "fabric.launch()\n",
    "model_manager = MlflowModelManager(fabric, cfg.model_manager.tracking_uri)\n",
    "\n",
    "model_info = mlflow.search_registered_models(filter_string=\"name='mlflow_example_agent'\")[-1]\n",
    "model_name = model_info.name\n",
    "print(\"Name:\", model_name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a New Model Version from Checkpoint\n",
    "\n",
    "Suppose to train a new PPO Agent in the CartPole-v1 environment and to obtain better results than before. You can register a new version of the model. To do this, we show another method to register models, not directly after training, but from a checkpoint.\n",
    "\n",
    "First of all, we need to run another experiment with different hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-12-07_12-46-10_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-12-07_12-46-10_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:240: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n",
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Rank-0: policy_step=1056, reward_env_3=19.0\n",
      "Rank-0: policy_step=1068, reward_env_0=18.0\n",
      "Rank-0: policy_step=1080, reward_env_2=29.0\n",
      "Rank-0: policy_step=1112, reward_env_1=26.0\n",
      "Rank-0: policy_step=1124, reward_env_2=11.0\n",
      "Rank-0: policy_step=1136, reward_env_0=17.0\n",
      "Rank-0: policy_step=1152, reward_env_3=24.0\n",
      "Rank-0: policy_step=1212, reward_env_0=19.0\n",
      "Rank-0: policy_step=1240, reward_env_2=29.0\n",
      "Rank-0: policy_step=1284, reward_env_0=18.0\n",
      "Rank-0: policy_step=1284, reward_env_1=43.0\n",
      "Rank-0: policy_step=1320, reward_env_2=20.0\n",
      "Rank-0: policy_step=1336, reward_env_3=46.0\n",
      "Rank-0: policy_step=1364, reward_env_1=20.0\n",
      "Rank-0: policy_step=1384, reward_env_0=25.0\n",
      "Rank-0: policy_step=1408, reward_env_3=18.0\n",
      "Rank-0: policy_step=1432, reward_env_2=28.0\n",
      "Rank-0: policy_step=1448, reward_env_0=16.0\n",
      "Rank-0: policy_step=1472, reward_env_1=27.0\n",
      "Rank-0: policy_step=1480, reward_env_3=18.0\n",
      "Rank-0: policy_step=1500, reward_env_2=17.0\n",
      "Rank-0: policy_step=1516, reward_env_0=17.0\n",
      "Rank-0: policy_step=1580, reward_env_0=16.0\n",
      "Rank-0: policy_step=1636, reward_env_1=41.0\n",
      "Rank-0: policy_step=1664, reward_env_2=41.0\n",
      "Rank-0: policy_step=1668, reward_env_0=22.0\n",
      "Rank-0: policy_step=1692, reward_env_3=53.0\n",
      "Rank-0: policy_step=1748, reward_env_1=28.0\n",
      "Rank-0: policy_step=1768, reward_env_0=25.0\n",
      "Rank-0: policy_step=1796, reward_env_2=33.0\n",
      "Rank-0: policy_step=1856, reward_env_1=27.0\n",
      "Rank-0: policy_step=1864, reward_env_2=17.0\n",
      "Rank-0: policy_step=1956, reward_env_0=47.0\n",
      "Rank-0: policy_step=1964, reward_env_2=25.0\n",
      "Rank-0: policy_step=2012, reward_env_1=39.0\n",
      "Rank-0: policy_step=2012, reward_env_2=12.0\n",
      "Rank-0: policy_step=2080, reward_env_1=17.0\n",
      "Rank-0: policy_step=2156, reward_env_3=116.0\n",
      "Rank-0: policy_step=2184, reward_env_1=26.0\n",
      "Rank-0: policy_step=2300, reward_env_1=29.0\n",
      "Rank-0: policy_step=2328, reward_env_0=93.0\n",
      "Rank-0: policy_step=2336, reward_env_2=81.0\n",
      "Rank-0: policy_step=2336, reward_env_3=45.0\n",
      "Rank-0: policy_step=2508, reward_env_3=43.0\n",
      "Rank-0: policy_step=2600, reward_env_2=66.0\n",
      "Rank-0: policy_step=2648, reward_env_1=87.0\n",
      "Rank-0: policy_step=2660, reward_env_0=83.0\n",
      "Rank-0: policy_step=2892, reward_env_1=61.0\n",
      "Rank-0: policy_step=2960, reward_env_3=113.0\n",
      "Rank-0: policy_step=3032, reward_env_0=93.0\n",
      "Rank-0: policy_step=3084, reward_env_1=48.0\n",
      "Rank-0: policy_step=3240, reward_env_2=160.0\n",
      "Rank-0: policy_step=3348, reward_env_0=79.0\n",
      "Rank-0: policy_step=3500, reward_env_2=65.0\n",
      "Rank-0: policy_step=3552, reward_env_1=117.0\n",
      "Rank-0: policy_step=3628, reward_env_1=19.0\n",
      "Rank-0: policy_step=3756, reward_env_3=199.0\n",
      "Rank-0: policy_step=3804, reward_env_2=76.0\n",
      "Rank-0: policy_step=3876, reward_env_1=62.0\n",
      "Rank-0: policy_step=3932, reward_env_0=146.0\n",
      "Rank-0: policy_step=4104, reward_env_1=57.0\n",
      "Rank-0: policy_step=4208, reward_env_3=113.0\n",
      "Rank-0: policy_step=4288, reward_env_1=46.0\n",
      "Rank-0: policy_step=4300, reward_env_2=124.0\n",
      "Rank-0: policy_step=4392, reward_env_1=26.0\n",
      "Rank-0: policy_step=4476, reward_env_3=67.0\n",
      "Rank-0: policy_step=4500, reward_env_2=50.0\n",
      "Rank-0: policy_step=4788, reward_env_0=214.0\n",
      "Rank-0: policy_step=4800, reward_env_1=102.0\n",
      "Rank-0: policy_step=5044, reward_env_2=136.0\n",
      "Rank-0: policy_step=5104, reward_env_3=157.0\n",
      "Rank-0: policy_step=5256, reward_env_0=117.0\n",
      "Rank-0: policy_step=5428, reward_env_1=157.0\n",
      "Rank-0: policy_step=5576, reward_env_0=80.0\n",
      "Rank-0: policy_step=5580, reward_env_2=134.0\n",
      "Rank-0: policy_step=5772, reward_env_3=167.0\n",
      "Rank-0: policy_step=6004, reward_env_1=144.0\n",
      "Rank-0: policy_step=6524, reward_env_3=188.0\n",
      "Rank-0: policy_step=6592, reward_env_1=147.0\n",
      "Rank-0: policy_step=6788, reward_env_2=302.0\n",
      "Rank-0: policy_step=7132, reward_env_1=135.0\n",
      "Rank-0: policy_step=7376, reward_env_0=450.0\n",
      "Rank-0: policy_step=7380, reward_env_3=214.0\n",
      "Rank-0: policy_step=7440, reward_env_2=163.0\n",
      "Rank-0: policy_step=8260, reward_env_1=282.0\n",
      "Rank-0: policy_step=8432, reward_env_2=248.0\n",
      "Rank-0: policy_step=8780, reward_env_3=350.0\n",
      "Rank-0: policy_step=9128, reward_env_1=217.0\n",
      "Rank-0: policy_step=9324, reward_env_2=223.0\n",
      "Rank-0: policy_step=9376, reward_env_0=500.0\n",
      "Rank-0: policy_step=9792, reward_env_3=253.0\n",
      "Rank-0: policy_step=10232, reward_env_1=276.0\n",
      "Rank-0: policy_step=10552, reward_env_0=294.0\n",
      "Rank-0: policy_step=10636, reward_env_2=328.0\n",
      "Rank-0: policy_step=10872, reward_env_3=270.0\n",
      "Rank-0: policy_step=11260, reward_env_1=257.0\n",
      "Rank-0: policy_step=11468, reward_env_0=229.0\n",
      "Rank-0: policy_step=11640, reward_env_2=251.0\n",
      "Rank-0: policy_step=11924, reward_env_3=263.0\n",
      "Rank-0: policy_step=12124, reward_env_1=216.0\n",
      "Rank-0: policy_step=12300, reward_env_0=208.0\n",
      "Rank-0: policy_step=12352, reward_env_1=57.0\n",
      "Rank-0: policy_step=12468, reward_env_2=207.0\n",
      "Rank-0: policy_step=12744, reward_env_3=205.0\n",
      "Rank-0: policy_step=12952, reward_env_0=163.0\n",
      "Rank-0: policy_step=13044, reward_env_2=144.0\n",
      "Rank-0: policy_step=13440, reward_env_1=272.0\n",
      "Rank-0: policy_step=13540, reward_env_1=25.0\n",
      "Rank-0: policy_step=13788, reward_env_3=261.0\n",
      "Rank-0: policy_step=13820, reward_env_2=194.0\n",
      "Rank-0: policy_step=14192, reward_env_3=101.0\n",
      "Rank-0: policy_step=14212, reward_env_2=98.0\n",
      "Rank-0: policy_step=14496, reward_env_3=76.0\n",
      "Rank-0: policy_step=14796, reward_env_2=146.0\n",
      "Rank-0: policy_step=14844, reward_env_0=473.0\n",
      "Rank-0: policy_step=15244, reward_env_3=187.0\n",
      "Rank-0: policy_step=15368, reward_env_0=131.0\n",
      "Rank-0: policy_step=15540, reward_env_1=500.0\n",
      "Rank-0: policy_step=15772, reward_env_2=244.0\n",
      "Test - Reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo algo.total_steps=16384 checkpoint.every=16384 logger@metric.logger=mlflow exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "import os\n",
    "\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg_ = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"algo.total_steps=16384\",\n",
    "            \"checkpoint.every=16384\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg_, resolve=True, throw_on_missing=True))\n",
    "run_algorithm(cfg)\n",
    "os.mkdir(f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/\")\n",
    "OmegaConf.save(cfg_, f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `./sheeprl_model_manager.py` script to take a checkpoint and register the models of the checkpoint.\n",
    "We want to retrieve the id of the last run, to associate the model to the correct run. We can take it from the UI (from the browser) or by retrieving it with the `mlflow.search_runs(experiment_ids=[exp.experiment_id])` instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/07 12:47:03 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-10-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'sheeprl'}\n",
      "Registered model 'mlflow_example_agent' already exists. Creating a new version of this model...\n",
      "2023/12/07 12:47:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example_agent, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example_agent with version 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'mlflow_example_agent'.\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.cli import registration\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl_model_manager.py checkpoint_path=</path/to/checkpoint.ckpt> \\\n",
    "# model_manager=ppo model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment' \\\n",
    "# run.id=<run_id>`\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "run_id = runs[\"run_id\"][0]\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"model_manager_config.yaml\",\n",
    "        overrides=[\n",
    "            # Substitute the checkpoint path with your /path/to/checkpoint.ckpt\n",
    "            \"checkpoint_path=./logs/runs/ppo/CartPole-v1/2023-12-07_12-46-10_mlflow_example_42/version_0/checkpoint/ckpt_16384_0.ckpt\",\n",
    "            \"model_manager=ppo\",\n",
    "            \"model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment'\",\n",
    "            f\"run.id={run_id}\",\n",
    "        ],\n",
    "    )\n",
    "registration(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, we can retrieve the new information of the registered model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example_agent\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 07/12/2023 12:46:10 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "## **Version 2**\n",
      "### Author: mmilesi\n",
      "### Date: 07/12/2023 12:47:04 CET\n",
      "### Description: \n",
      "New PPO Agent version trained in CartPole-v1 environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 2\n"
     ]
    }
   ],
   "source": [
    "model_info = mlflow.search_registered_models(filter_string=f\"name='{model_name}'\")[-1]\n",
    "print(\"Name:\", model_info.name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging the Model\n",
    "After registering the model, we can transition the model to a new stage. We can transition the model to the `\"staging\"` stage with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitioning model mlflow_example_agent version 2 from None to staging\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1701949624027, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 07/12/2023 12:47:04 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'), last_updated_timestamp=1701949660778, name='mlflow_example_agent', run_id='eefbe09e8815463eaa83c6542cbc36c7', run_link='', source='mlflow-artifacts:/242317125620601262/eefbe09e8815463eaa83c6542cbc36c7/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.transition_model(\n",
    "    model_name=\"mlflow_example_agent\",\n",
    "    version=latest_version.version,\n",
    "    stage=\"staging\",\n",
    "    description=\"Staging Model for demo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Model\n",
    "You can download the registered models and load them with the `torch.load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model mlflow_example_agent version 2 from mlflow-artifacts:/242317125620601262/eefbe09e8815463eaa83c6542cbc36c7/artifacts/agent to ./models/ppo-agent-cartpole\n",
      "Creating output path ./models/ppo-agent-cartpole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 64.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOAgent(\n",
       "  (feature_extractor): MultiEncoder(\n",
       "    (mlp_encoder): MLPEncoder(\n",
       "      (model): MLP(\n",
       "        (_model): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "          (1): Tanh()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Tanh()\n",
       "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (critic): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (actor_backbone): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (actor_heads): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "download_path = \"./models/ppo-agent-cartpole\"\n",
    "model_manager.download_model(model_name, latest_version.version, download_path)\n",
    "agent = torch.load(\"models/ppo-agent-cartpole/agent/data/model.pth\")\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Best Models\n",
    "Another possibility is to register the best models of a specific experiment. Let us suppose we want to register the best model of the two experiments we ran before: the only thing we have to do is to call the `model_manager.register_best_models()` function by specifying the `experiment_name`, the `metric`, and the `models_info` (a python dictionary containing the name, the path, the description and the tags of the models we want to register), as shown below.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> If your experiment contains different agents, and each agent has different model paths, then you have to specify in the `models_info` all the models you want to register (i.e., the union of the models of all the agents). The MLFlow model manager will automatically select the correct models for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ppo_agent_cartpole_best_reward'.\n",
      "2023/12/07 12:47:55 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ppo_agent_cartpole_best_reward, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model ppo_agent_cartpole_best_reward with version 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'ppo_agent_cartpole_best_reward'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agent': <ModelVersion: aliases=[], creation_timestamp=1701949675859, current_stage='None', description='', last_updated_timestamp=1701949675859, name='ppo_agent_cartpole_best_reward', run_id='eefbe09e8815463eaa83c6542cbc36c7', run_link='', source='mlflow-artifacts:/242317125620601262/eefbe09e8815463eaa83c6542cbc36c7/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='1'>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_info = {\n",
    "    \"agent\": {\n",
    "        \"name\": \"ppo_agent_cartpole_best_reward\",\n",
    "        \"path\": \"agent\",\n",
    "        \"tags\": {},\n",
    "        \"description\": \"The best PPO Agent in CartPole environment.\",\n",
    "    }\n",
    "}\n",
    "model_manager.register_best_models(\"mlflow_example\", models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Model\n",
    "Finally, you can delete registered models you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model named mlflow_example_agent with version 1 does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RegisteredModel: aliases={}, creation_timestamp=1701949570369, description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 1**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 07/12/2023 12:46:10 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'PPO Agent in CartPole-v1 Environment\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 07/12/2023 12:47:04 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 07/12/2023 12:47:40 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'\n",
       " '## **Deletion:**\\n'\n",
       " '### Version 1 from stage: None\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 07/12/2023 12:48:36 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Delete model version 1\\n'), last_updated_timestamp=1701949716092, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1701949624027, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 07/12/2023 12:47:04 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 07/12/2023 12:47:40 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'), last_updated_timestamp=1701949660803, name='mlflow_example_agent', run_id='eefbe09e8815463eaa83c6542cbc36c7', run_link='', source='mlflow-artifacts:/242317125620601262/eefbe09e8815463eaa83c6542cbc36c7/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>], name='mlflow_example_agent', tags={}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.delete_model(\n",
    "    model_name, int(latest_version.version) - 1, f\"Delete model version {int(latest_version.version)-1}\"\n",
    ")\n",
    "mlflow.search_registered_models(filter_string=\"name='mlflow_example_agent'\")[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
