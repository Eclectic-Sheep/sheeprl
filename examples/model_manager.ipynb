{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Manager\n",
    "\n",
    "In this notebook we prenset the [MlflowModelManager](../sheeprl/utils/model_manager.py) and a possible use.\n",
    "It includes methods such as:\n",
    "* Register the model\n",
    "* Retrieve latest version\n",
    "* Transition the model to a new stage\n",
    "* Delete the model\n",
    "\n",
    "First of all, we need to run Mlflow server with artifact store. You can find the instructions for running Mlflow server [here](https://mlflow.org/docs/latest/tracking.html#tracking-ui). Let's open a new terminal and run the following command:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> This is one of the possibilities, you could have the server running on another machine, so you just need to set the `tracking_uri` parameter properly.\n",
    "\n",
    "### Running the Experiment and Registering the Model\n",
    "Second, we launch an experiment, so we need to retrieve the configs and execute the `run_algorithm` function. We train a PPO agent in the CartPole-v1 environment for few steps (we do not want to reach the best performance, but we want to show how SheepRL interprets model management for reinforcement learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Experiment with name mlflow_example not found. Creating it.\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-11-24_12-19-35_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-11-24_12-19-35_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:231: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Test - Reward: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/24 12:19:48 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-10-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'sheeprl'}\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'mlflow_example'.\n",
      "2023/11/24 12:19:48 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example with version 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'mlflow_example'.\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from sheeprl.utils.utils import dotdict\n",
    "from sheeprl.cli import run_algorithm\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo total_steps=1024 model_manager.disabled=False logger@metric.logger=mlflow checkpoint.every=1024 exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"total_steps=1024\",\n",
    "            \"model_manager.disabled=False\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"checkpoint.every=1024\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True))\n",
    "run_algorithm(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Experiment Info\n",
    "\n",
    "The experiment is logged on MLFlow, and we can retrieve it and the just  with the following instructions. Moreover, given the experiment it is possible to retrieve all the runs with the `mlflow.search_runs()` function.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> You can check these information also from a browser, by entering the MLFlow address in a browser, e.g., `http://localhost:5000` if you are running mlflow locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: <Experiment: artifact_location='mlflow-artifacts:/541057322835557968', creation_time=1700824776648, experiment_id='541057322835557968', last_update_time=1700824776648, lifecycle_stage='active', name='mlflow_example', tags={}>\n",
      "Experiment (541057322835557968) runs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.Time/sps_env_interaction</th>\n",
       "      <th>metrics.Loss/policy_loss</th>\n",
       "      <th>metrics.Loss/entropy_loss</th>\n",
       "      <th>metrics.Loss/value_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>params.algo/clip_coef</th>\n",
       "      <th>params.exp_name</th>\n",
       "      <th>params.env/max_episode_steps</th>\n",
       "      <th>params.metric/aggregator/metrics/Loss/entropy_loss/sync_on_compute</th>\n",
       "      <th>params.metric/aggregator/metrics/Loss/value_loss/_target_</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86ed7648208c47cbab275fa7e612e456</td>\n",
       "      <td>541057322835557968</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>mlflow-artifacts:/541057322835557968/86ed76482...</td>\n",
       "      <td>2023-11-24 11:19:36.855000+00:00</td>\n",
       "      <td>2023-11-24 11:19:48.951000+00:00</td>\n",
       "      <td>329.871509</td>\n",
       "      <td>-5.913153</td>\n",
       "      <td>-0.687031</td>\n",
       "      <td>36.166283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>mlflow_example</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>torchmetrics.MeanMetric</td>\n",
       "      <td>mmilesi</td>\n",
       "      <td>ppo_CartPole-v1_2023-11-24_12-19-35</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>/home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...</td>\n",
       "      <td>[{\"run_id\": \"86ed7648208c47cbab275fa7e612e456\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id       experiment_id    status  \\\n",
       "0  86ed7648208c47cbab275fa7e612e456  541057322835557968  FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  mlflow-artifacts:/541057322835557968/86ed76482...   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2023-11-24 11:19:36.855000+00:00 2023-11-24 11:19:48.951000+00:00   \n",
       "\n",
       "   metrics.Time/sps_env_interaction  metrics.Loss/policy_loss  \\\n",
       "0                        329.871509                 -5.913153   \n",
       "\n",
       "   metrics.Loss/entropy_loss  metrics.Loss/value_loss  ...  \\\n",
       "0                  -0.687031                36.166283  ...   \n",
       "\n",
       "   params.algo/clip_coef  params.exp_name  params.env/max_episode_steps  \\\n",
       "0                    0.2   mlflow_example                          None   \n",
       "\n",
       "   params.metric/aggregator/metrics/Loss/entropy_loss/sync_on_compute  \\\n",
       "0                                              False                    \n",
       "\n",
       "   params.metric/aggregator/metrics/Loss/value_loss/_target_  \\\n",
       "0                            torchmetrics.MeanMetric           \n",
       "\n",
       "   tags.mlflow.user                  tags.mlflow.runName  \\\n",
       "0           mmilesi  ppo_CartPole-v1_2023-11-24_12-19-35   \n",
       "\n",
       "  tags.mlflow.source.type                            tags.mlflow.source.name  \\\n",
       "0                   LOCAL  /home/mmilesi/miniconda3/envs/sheeprl/lib/pyth...   \n",
       "\n",
       "                       tags.mlflow.log-model.history  \n",
       "0  [{\"run_id\": \"86ed7648208c47cbab275fa7e612e456\"...  \n",
       "\n",
       "[1 rows x 131 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(cfg.metric.logger.tracking_uri)\n",
    "exp = mlflow.get_experiment_by_name(\"mlflow_example\")\n",
    "print(\"Experiment:\", exp)\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "print(f\"Experiment ({exp.experiment_id}) runs:\")\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Model Info\n",
    "Since we set the `model_manager.disabled` to `False` the PPO Agent is registered in MLFLow, we can get its information with the following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 24/11/2023 12:19:48 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 1\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.utils.model_manager import MlflowModelManager\n",
    "from lightning import Fabric\n",
    "\n",
    "fabric = Fabric(devices=1, accelerator=cfg.fabric.accelerator, precision=cfg.fabric.precision)\n",
    "fabric.launch()\n",
    "model_manager = MlflowModelManager(fabric, cfg.model_manager.tracking_uri)\n",
    "\n",
    "model_info = mlflow.search_registered_models(filter_string=\"name='mlflow_example'\")[-1]\n",
    "model_name = model_info.name\n",
    "print(\"Name:\", model_name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a New Model Version from Checkpoint\n",
    "\n",
    "Suppose to train a new PPO Agent in the CartPole-v1 environment and to obtain better results than before. You can register a new version of the model. To do this, we show another method to register models, not directly after training, but from a checkpoint.\n",
    "\n",
    "First of all, we need to run another experiment with different hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/utils/logger.py:79: UserWarning: Missing logger folder: logs/runs/ppo/CartPole-v1/2023-11-24_12-19-49_mlflow_example_42\n",
      "  warnings.warn(\"Missing logger folder: %s\" % save_dir, UserWarning)\n",
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/gymnasium/experimental/wrappers/rendering.py:166: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/mmilesi/repos/sheeprl/examples/logs/runs/ppo/CartPole-v1/2023-11-24_12-19-49_mlflow_example_42/version_0/train_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/mmilesi/repos/sheeprl/sheeprl/algos/ppo/ppo.py:231: UserWarning: The metric.log_every parameter (5000) is not a multiple of the policy_steps_per_update value (512), so the metrics will be logged at the nearest greater multiple of the policy_steps_per_update value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CNN keys: []\n",
      "Encoder MLP keys: ['state']\n",
      "Rank-0: policy_step=44, reward_env_1=11.0\n",
      "Rank-0: policy_step=64, reward_env_3=16.0\n",
      "Rank-0: policy_step=92, reward_env_2=23.0\n",
      "Rank-0: policy_step=104, reward_env_3=10.0\n",
      "Rank-0: policy_step=108, reward_env_0=27.0\n",
      "Rank-0: policy_step=128, reward_env_1=21.0\n",
      "Rank-0: policy_step=144, reward_env_2=13.0\n",
      "Rank-0: policy_step=160, reward_env_0=13.0\n",
      "Rank-0: policy_step=168, reward_env_1=10.0\n",
      "Rank-0: policy_step=168, reward_env_3=16.0\n",
      "Rank-0: policy_step=260, reward_env_1=23.0\n",
      "Rank-0: policy_step=284, reward_env_0=31.0\n",
      "Rank-0: policy_step=300, reward_env_2=39.0\n",
      "Rank-0: policy_step=352, reward_env_1=23.0\n",
      "Rank-0: policy_step=376, reward_env_0=23.0\n",
      "Rank-0: policy_step=424, reward_env_0=12.0\n",
      "Rank-0: policy_step=444, reward_env_1=23.0\n",
      "Rank-0: policy_step=484, reward_env_0=15.0\n",
      "Rank-0: policy_step=536, reward_env_1=23.0\n",
      "Rank-0: policy_step=556, reward_env_3=97.0\n",
      "Rank-0: policy_step=592, reward_env_2=73.0\n",
      "Rank-0: policy_step=600, reward_env_1=16.0\n",
      "Rank-0: policy_step=636, reward_env_0=38.0\n",
      "Rank-0: policy_step=644, reward_env_3=22.0\n",
      "Rank-0: policy_step=660, reward_env_1=15.0\n",
      "Rank-0: policy_step=672, reward_env_2=20.0\n",
      "Rank-0: policy_step=720, reward_env_2=12.0\n",
      "Rank-0: policy_step=728, reward_env_0=23.0\n",
      "Rank-0: policy_step=792, reward_env_3=37.0\n",
      "Rank-0: policy_step=796, reward_env_1=34.0\n",
      "Rank-0: policy_step=800, reward_env_0=18.0\n",
      "Rank-0: policy_step=800, reward_env_2=20.0\n",
      "Rank-0: policy_step=848, reward_env_1=13.0\n",
      "Rank-0: policy_step=856, reward_env_2=14.0\n",
      "Rank-0: policy_step=868, reward_env_3=19.0\n",
      "Rank-0: policy_step=916, reward_env_2=15.0\n",
      "Rank-0: policy_step=920, reward_env_0=30.0\n",
      "Rank-0: policy_step=932, reward_env_3=16.0\n",
      "Rank-0: policy_step=948, reward_env_1=25.0\n",
      "Rank-0: policy_step=964, reward_env_2=12.0\n",
      "Rank-0: policy_step=980, reward_env_3=12.0\n",
      "Rank-0: policy_step=996, reward_env_0=19.0\n",
      "Rank-0: policy_step=1008, reward_env_1=15.0\n",
      "Rank-0: policy_step=1056, reward_env_3=19.0\n",
      "Rank-0: policy_step=1068, reward_env_0=18.0\n",
      "Rank-0: policy_step=1080, reward_env_2=29.0\n",
      "Rank-0: policy_step=1112, reward_env_1=26.0\n",
      "Rank-0: policy_step=1124, reward_env_2=11.0\n",
      "Rank-0: policy_step=1136, reward_env_0=17.0\n",
      "Rank-0: policy_step=1152, reward_env_3=24.0\n",
      "Rank-0: policy_step=1212, reward_env_0=19.0\n",
      "Rank-0: policy_step=1240, reward_env_2=29.0\n",
      "Rank-0: policy_step=1284, reward_env_0=18.0\n",
      "Rank-0: policy_step=1284, reward_env_1=43.0\n",
      "Rank-0: policy_step=1320, reward_env_2=20.0\n",
      "Rank-0: policy_step=1336, reward_env_3=46.0\n",
      "Rank-0: policy_step=1364, reward_env_1=20.0\n",
      "Rank-0: policy_step=1384, reward_env_0=25.0\n",
      "Rank-0: policy_step=1408, reward_env_3=18.0\n",
      "Rank-0: policy_step=1432, reward_env_2=28.0\n",
      "Rank-0: policy_step=1448, reward_env_0=16.0\n",
      "Rank-0: policy_step=1472, reward_env_1=27.0\n",
      "Rank-0: policy_step=1480, reward_env_3=18.0\n",
      "Rank-0: policy_step=1500, reward_env_2=17.0\n",
      "Rank-0: policy_step=1516, reward_env_0=17.0\n",
      "Rank-0: policy_step=1580, reward_env_0=16.0\n",
      "Rank-0: policy_step=1644, reward_env_1=43.0\n",
      "Rank-0: policy_step=1664, reward_env_2=41.0\n",
      "Rank-0: policy_step=1668, reward_env_0=22.0\n",
      "Rank-0: policy_step=1692, reward_env_3=53.0\n",
      "Rank-0: policy_step=1744, reward_env_1=25.0\n",
      "Rank-0: policy_step=1768, reward_env_0=25.0\n",
      "Rank-0: policy_step=1796, reward_env_2=33.0\n",
      "Rank-0: policy_step=1864, reward_env_2=17.0\n",
      "Rank-0: policy_step=1956, reward_env_0=47.0\n",
      "Rank-0: policy_step=1960, reward_env_1=54.0\n",
      "Rank-0: policy_step=1964, reward_env_2=25.0\n",
      "Rank-0: policy_step=2012, reward_env_2=12.0\n",
      "Rank-0: policy_step=2040, reward_env_1=20.0\n",
      "Rank-0: policy_step=2152, reward_env_1=28.0\n",
      "Rank-0: policy_step=2164, reward_env_3=118.0\n",
      "Rank-0: policy_step=2196, reward_env_1=11.0\n",
      "Rank-0: policy_step=2260, reward_env_2=62.0\n",
      "Rank-0: policy_step=2268, reward_env_0=78.0\n",
      "Rank-0: policy_step=2276, reward_env_1=20.0\n",
      "Rank-0: policy_step=2308, reward_env_3=36.0\n",
      "Rank-0: policy_step=2492, reward_env_2=58.0\n",
      "Rank-0: policy_step=2520, reward_env_0=63.0\n",
      "Rank-0: policy_step=2584, reward_env_2=23.0\n",
      "Rank-0: policy_step=2608, reward_env_1=83.0\n",
      "Rank-0: policy_step=2632, reward_env_3=81.0\n",
      "Rank-0: policy_step=2692, reward_env_0=43.0\n",
      "Rank-0: policy_step=2780, reward_env_1=43.0\n",
      "Rank-0: policy_step=2796, reward_env_2=53.0\n",
      "Rank-0: policy_step=2928, reward_env_0=59.0\n",
      "Rank-0: policy_step=2940, reward_env_2=36.0\n",
      "Rank-0: policy_step=2944, reward_env_3=78.0\n",
      "Rank-0: policy_step=3012, reward_env_1=58.0\n",
      "Rank-0: policy_step=3080, reward_env_0=38.0\n",
      "Rank-0: policy_step=3224, reward_env_3=70.0\n",
      "Rank-0: policy_step=3284, reward_env_1=68.0\n",
      "Rank-0: policy_step=3284, reward_env_2=86.0\n",
      "Rank-0: policy_step=3288, reward_env_0=52.0\n",
      "Rank-0: policy_step=3480, reward_env_2=49.0\n",
      "Rank-0: policy_step=3548, reward_env_3=81.0\n",
      "Rank-0: policy_step=3588, reward_env_1=76.0\n",
      "Rank-0: policy_step=3712, reward_env_2=58.0\n",
      "Rank-0: policy_step=3840, reward_env_1=63.0\n",
      "Rank-0: policy_step=3928, reward_env_2=54.0\n",
      "Rank-0: policy_step=3956, reward_env_3=102.0\n",
      "Rank-0: policy_step=4016, reward_env_0=182.0\n",
      "Rank-0: policy_step=4096, reward_env_1=64.0\n",
      "Rank-0: policy_step=4252, reward_env_1=39.0\n",
      "Rank-0: policy_step=4256, reward_env_0=60.0\n",
      "Rank-0: policy_step=4316, reward_env_3=90.0\n",
      "Rank-0: policy_step=4416, reward_env_1=41.0\n",
      "Rank-0: policy_step=4476, reward_env_2=137.0\n",
      "Rank-0: policy_step=4776, reward_env_1=90.0\n",
      "Rank-0: policy_step=4800, reward_env_0=136.0\n",
      "Rank-0: policy_step=4836, reward_env_2=90.0\n",
      "Rank-0: policy_step=4920, reward_env_3=151.0\n",
      "Rank-0: policy_step=5176, reward_env_1=100.0\n",
      "Rank-0: policy_step=5308, reward_env_0=127.0\n",
      "Rank-0: policy_step=5332, reward_env_2=124.0\n",
      "Rank-0: policy_step=5388, reward_env_3=117.0\n",
      "Rank-0: policy_step=5556, reward_env_0=62.0\n",
      "Rank-0: policy_step=5736, reward_env_3=87.0\n",
      "Rank-0: policy_step=5744, reward_env_1=142.0\n",
      "Rank-0: policy_step=5832, reward_env_2=125.0\n",
      "Rank-0: policy_step=5968, reward_env_0=103.0\n",
      "Rank-0: policy_step=6168, reward_env_1=106.0\n",
      "Rank-0: policy_step=6272, reward_env_3=134.0\n",
      "Rank-0: policy_step=6340, reward_env_2=127.0\n",
      "Rank-0: policy_step=6408, reward_env_0=110.0\n",
      "Rank-0: policy_step=6756, reward_env_1=147.0\n",
      "Rank-0: policy_step=6856, reward_env_3=146.0\n",
      "Rank-0: policy_step=6876, reward_env_2=134.0\n",
      "Rank-0: policy_step=6920, reward_env_0=128.0\n",
      "Rank-0: policy_step=7308, reward_env_1=138.0\n",
      "Rank-0: policy_step=7644, reward_env_3=197.0\n",
      "Rank-0: policy_step=7688, reward_env_2=203.0\n",
      "Rank-0: policy_step=7700, reward_env_0=195.0\n",
      "Rank-0: policy_step=8080, reward_env_1=193.0\n",
      "Rank-0: policy_step=8636, reward_env_2=237.0\n",
      "Rank-0: policy_step=8644, reward_env_3=250.0\n",
      "Rank-0: policy_step=8756, reward_env_0=264.0\n",
      "Rank-0: policy_step=8868, reward_env_1=197.0\n",
      "Rank-0: policy_step=9728, reward_env_2=273.0\n",
      "Rank-0: policy_step=9864, reward_env_3=305.0\n",
      "Rank-0: policy_step=10756, reward_env_0=500.0\n",
      "Rank-0: policy_step=10848, reward_env_1=495.0\n",
      "Rank-0: policy_step=11124, reward_env_2=349.0\n",
      "Rank-0: policy_step=11472, reward_env_3=402.0\n",
      "Rank-0: policy_step=12756, reward_env_0=500.0\n",
      "Rank-0: policy_step=12848, reward_env_1=500.0\n",
      "Rank-0: policy_step=13124, reward_env_2=500.0\n",
      "Rank-0: policy_step=13472, reward_env_3=500.0\n",
      "Rank-0: policy_step=14324, reward_env_1=369.0\n",
      "Rank-0: policy_step=14532, reward_env_0=444.0\n",
      "Rank-0: policy_step=14796, reward_env_3=331.0\n",
      "Rank-0: policy_step=15112, reward_env_2=497.0\n",
      "Rank-0: policy_step=15924, reward_env_1=400.0\n",
      "Rank-0: policy_step=16000, reward_env_0=367.0\n",
      "Rank-0: policy_step=16044, reward_env_2=233.0\n",
      "Rank-0: policy_step=16152, reward_env_3=339.0\n",
      "Test - Reward: 392.0\n"
     ]
    }
   ],
   "source": [
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl.py exp=ppo total_steps=16384 checkpoint.every=16384 logger@metric.logger=mlflow exp_name=mlflow_example metric.logger.tracking_uri=\"http://localhost:5000\"`\n",
    "import os\n",
    "\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg_ = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        overrides=[\n",
    "            \"exp=ppo\",\n",
    "            \"total_steps=16384\",\n",
    "            \"checkpoint.every=16384\",\n",
    "            \"logger@metric.logger=mlflow\",\n",
    "            \"exp_name=mlflow_example\",\n",
    "            \"metric.logger.tracking_uri=http://localhost:5000\",\n",
    "        ],\n",
    "    )\n",
    "    cfg = dotdict(OmegaConf.to_container(cfg_, resolve=True, throw_on_missing=True))\n",
    "run_algorithm(cfg)\n",
    "os.mkdir(f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/\")\n",
    "OmegaConf.save(cfg_, f\"./logs/runs/{cfg.root_dir}/{cfg.run_name}/.hydra/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `./sheeprl_model_manager.py` script to take a checkpoint and register the models of the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/24 12:20:40 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2023-10-28; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'sheeprl'}\n",
      "Registered model 'mlflow_example' already exists. Creating a new version of this model...\n",
      "2023/11/24 12:20:40 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_example, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model mlflow_example with version 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'mlflow_example'.\n"
     ]
    }
   ],
   "source": [
    "from sheeprl.cli import registration\n",
    "\n",
    "# To retrieve the configs, we can simulate the cli command\n",
    "# `python sheeprl_model_manager.py checkpoint_path=/path/to/checkpoint.ckpt \\\n",
    "# model_manager=ppo model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment'`\n",
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../sheeprl/configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"model_manager_config.yaml\",\n",
    "        overrides=[\n",
    "            # Substitute the checkpoint path with your /path/to/checkpoint.ckpt\n",
    "            \"checkpoint_path=./logs/runs/ppo/CartPole-v1/2023-11-24_12-19-49_mlflow_example_42/version_0/checkpoint/ckpt_16384_0.ckpt\",\n",
    "            \"model_manager=ppo\",\n",
    "            \"model_manager.models.agent.description='New PPO Agent version trained in CartPole-v1 environment'\",\n",
    "        ],\n",
    "    )\n",
    "registration(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, we can retrieve the new information of the registred model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow_example\n",
      "Description: # MODEL CHANGELOG\n",
      "## **Version 1**\n",
      "### Author: mmilesi\n",
      "### Date: 24/11/2023 12:19:48 CET\n",
      "### Description: \n",
      "PPO Agent in CartPole-v1 Environment\n",
      "## **Version 2**\n",
      "### Author: mmilesi\n",
      "### Date: 24/11/2023 12:20:40 CET\n",
      "### Description: \n",
      "New PPO Agent version trained in CartPole-v1 environment\n",
      "\n",
      "Tags: {}\n",
      "Latest Version: 2\n"
     ]
    }
   ],
   "source": [
    "model_info = mlflow.search_registered_models(filter_string=f\"name='{model_name}'\")[-1]\n",
    "print(\"Name:\", model_info.name)\n",
    "print(\"Description:\", model_info.description)\n",
    "print(\"Tags:\", model_info.tags)\n",
    "latest_version = model_manager.get_latest_version(model_info.name)\n",
    "print(\"Latest Version:\", latest_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging the Model\n",
    "After registering the model, we can transition the model to a new stage. We can transition the model to staging stage with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitioning model mlflow_example version 2 from None to staging\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1700824840570, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 24/11/2023 12:20:40 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'), last_updated_timestamp=1700824844145, name='mlflow_example', run_id='1ecf72c9e0dd43a68e13c19919b7258b', run_link='', source='mlflow-artifacts:/765086633400039374/1ecf72c9e0dd43a68e13c19919b7258b/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.transition_model(\n",
    "    model_name=\"mlflow_example\", version=latest_version.version, stage=\"staging\", description=\"Staging Model for demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Model\n",
    "You can download the registered models and load with the `torch.load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilesi/miniconda3/envs/sheeprl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model mlflow_example version 2 from mlflow-artifacts:/765086633400039374/1ecf72c9e0dd43a68e13c19919b7258b/artifacts/agent to ./models/ppo-agent-cartpole\n",
      "Creating output path ./models/ppo-agent-cartpole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 275.64it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOAgent(\n",
       "  (feature_extractor): MultiEncoder(\n",
       "    (mlp_encoder): MLPEncoder(\n",
       "      (model): MLP(\n",
       "        (_model): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "          (1): Tanh()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Tanh()\n",
       "          (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (critic): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (actor_backbone): MLP(\n",
       "    (_model): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (actor_heads): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "download_path = \"./models/ppo-agent-cartpole\"\n",
    "model_manager.download_model(model_name, latest_version.version, download_path)\n",
    "agent = torch.load(\"models/ppo-agent-cartpole/agent/data/model.pth\")\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Model\n",
    "Finally, you can delete registered models you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model mlflow_example version 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RegisteredModel: aliases={}, creation_timestamp=1700824788962, description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 1**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 24/11/2023 12:19:48 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'PPO Agent in CartPole-v1 Environment\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 24/11/2023 12:20:40 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 24/11/2023 12:20:44 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'\n",
       " '## **Deletion:**\\n'\n",
       " '### Version 1 from stage: None\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 24/11/2023 12:21:01 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Delete model version 1\\n'), last_updated_timestamp=1700824861631, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1700824840570, current_stage='Staging', description=('# MODEL CHANGELOG\\n'\n",
       " '## **Version 2**\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 24/11/2023 12:20:40 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'New PPO Agent version trained in CartPole-v1 environment\\n'\n",
       " '## **Transition:**\\n'\n",
       " '### Version 2 from None to Staging\\n'\n",
       " '### Author: mmilesi\\n'\n",
       " '### Date: 24/11/2023 12:20:44 CET\\n'\n",
       " '### Description: \\n'\n",
       " 'Staging Model for demo\\n'), last_updated_timestamp=1700824844173, name='mlflow_example', run_id='1ecf72c9e0dd43a68e13c19919b7258b', run_link='', source='mlflow-artifacts:/765086633400039374/1ecf72c9e0dd43a68e13c19919b7258b/artifacts/agent', status='READY', status_message='', tags={}, user_id='', version='2'>], name='mlflow_example', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.delete_model(\n",
    "    model_name, int(latest_version.version) - 1, f\"Delete model version {int(latest_version.version)-1}\"\n",
    ")\n",
    "mlflow.search_registered_models(filter_string=\"name='mlflow_example'\")[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
