{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dreamer V3 imagination  + reconstruct observations\n",
    "\n",
    "This notebook is updated to SheepRL version [v0.5.5](https://github.com/Eclectic-Sheep/sheeprl/tree/release/v0.5.5). It provides a way to obtain the imagined and reconstructed frames of the Dreamer V3 agent. By default, a GIF is created, but one can create a video from them.\n",
    "\n",
    "This notebook is organized as follows:\n",
    "1. Agent and environment creation from checkpoint\n",
    "2. Buffer initialization: the buffers necessary for saving collected experiences, imagined and reconstructed observations.\n",
    "3. Initial environment interaction: the agent plays a number of `initial_steps` in the environment. The collected steps are saved in the `rb_initial` buffer.\n",
    "4. Imagination and reconstruction: we set `imagination_steps` as the number of imagination/reconstruction steps we want the agent to perform. So the agent starts from the $\\texttt{initial\\_step} - \\texttt{imagination\\_steps}$ step and the imagined/reconstructed observations are computed for the steps in $[\\texttt{initial\\_step} - \\texttt{imagination\\_steps}, \\texttt{initial\\_step}]$.\n",
    "5. GIF Creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Import of libraries\n",
    "\n",
    "It is necessary to install the `torchvision` package to exploit some utils for image manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pathlib\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from lightning.fabric import Fabric\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "\n",
    "from sheeprl.algos.dreamer_v3.agent import build_agent\n",
    "from sheeprl.data.buffers import SequentialReplayBuffer\n",
    "from sheeprl.utils.env import make_env\n",
    "from sheeprl.utils.utils import dotdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Environment initialization\n",
    "\n",
    "The checkpoit is taken and the number of environments is set to 1 since we want to observe the ability of the agent to reconstruct the observations or to imagine future steps.\n",
    "\n",
    "The code for creating the environment and initializing the agent is the same of the `sheeprl/algos/dreamer_v3/dreamer_v3.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of your checkpoint\n",
    "ckpt_path = pathlib.Path(\"/path/to/your/checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "fabric = Fabric(accelerator=\"cuda\", devices=1)\n",
    "state = fabric.load(ckpt_path)\n",
    "cfg = dotdict(OmegaConf.to_container(OmegaConf.load(ckpt_path.parent.parent / \"config.yaml\"), resolve=True))\n",
    "\n",
    "# The number of environments is set to 1\n",
    "cfg.env.num_envs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [\n",
    "        make_env(\n",
    "            cfg,\n",
    "            cfg.seed + 0 * cfg.env.num_envs + i,\n",
    "            0 * cfg.env.num_envs,\n",
    "            \"./imagination\",\n",
    "            \"imagination\",\n",
    "            vector_env_idx=i,\n",
    "        )\n",
    "        for i in range(cfg.env.num_envs)\n",
    "    ]\n",
    ")\n",
    "action_space = envs.single_action_space\n",
    "observation_space = envs.single_observation_space\n",
    "obs_keys = cfg.algo.cnn_keys.encoder + cfg.algo.mlp_keys.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_continuous = isinstance(action_space, gym.spaces.Box)\n",
    "is_multidiscrete = isinstance(action_space, gym.spaces.MultiDiscrete)\n",
    "actions_dim = tuple(\n",
    "    action_space.shape if is_continuous else (action_space.nvec.tolist() if is_multidiscrete else [action_space.n])\n",
    ")\n",
    "world_model, actor, critic, critic_target, player = build_agent(\n",
    "    fabric,\n",
    "    actions_dim,\n",
    "    is_continuous,\n",
    "    cfg,\n",
    "    observation_space,\n",
    "    state[\"world_model\"],\n",
    "    state[\"actor\"],\n",
    "    state[\"critic\"],\n",
    "    state[\"target_critic\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer initialization\n",
    "\n",
    "Here, buffers are initialized where the steps played by the agent will be saved:\n",
    "- `rb_initial`: used to save the steps played during the initial environment interaction. (dimension equal to `intial_steps`).\n",
    "- `rb_play`: contains the reconstructed observations of dimension `imagination_steps`.\n",
    "- `rb_imagination`: contains the imagined steps. \n",
    "\n",
    "The reconstructed and the imagined steps are from the $\\texttt{initial\\_step} - \\texttt{imagination\\_steps}$ step to the $\\texttt{initial\\_step}$ step.\n",
    "\n",
    "Tips:\n",
    "- If you want to observe how the agent behaves in the first steps of the episode, set the value of `initial_steps` low.\n",
    "- The `imagination_steps` should not be too high, since during training the agent imagines 15 steps ahead.\n",
    "- If you are not interested in the imagination part, then you can set `initial_steps` equal to `imagination_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_steps = 200  # set according to your environment.\n",
    "imagination_steps = 45  # number of imagination steps, must be lower than or equal to the `initial_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_rewards_fn = lambda r: torch.tanh(r) if cfg.env.clip_rewards else r\n",
    "rb_initial = SequentialReplayBuffer(initial_steps, cfg.env.num_envs)\n",
    "rb_play = SequentialReplayBuffer(imagination_steps, cfg.env.num_envs)\n",
    "rb_imagination = SequentialReplayBuffer(imagination_steps, cfg.env.num_envs)\n",
    "step_data = {}\n",
    "player.init_states()\n",
    "obs = envs.reset(seed=cfg.seed)[0]\n",
    "for k in obs_keys:\n",
    "    step_data[k] = obs[k][np.newaxis]\n",
    "step_data[\"dones\"] = np.zeros((1, cfg.env.num_envs, 1))\n",
    "step_data[\"rewards\"] = np.zeros((1, cfg.env.num_envs, 1))\n",
    "step_data[\"is_first\"] = np.ones_like(step_data[\"dones\"])\n",
    "step_data[\"stochastic_state\"] = player.stochastic_state.detach().cpu().numpy()\n",
    "step_data[\"recurrent_state\"] = player.recurrent_state.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment interaction\n",
    "\n",
    "The agent plays for `initial_steps` and save the collected steps into the `rb_initial` buffer. Latent states computed by the agent during this phase are saved. The Observations are then reconstructed from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_list = []\n",
    "# play for `initial_steps` steps\n",
    "for i in range(initial_steps):\n",
    "    with torch.no_grad():\n",
    "        preprocessed_obs = {}\n",
    "        for k, v in obs.items():\n",
    "            preprocessed_obs[k] = torch.as_tensor(v[np.newaxis], dtype=torch.float32, device=fabric.device)\n",
    "            if k in cfg.algo.cnn_keys.encoder:\n",
    "                preprocessed_obs[k] = preprocessed_obs[k] / 255.0\n",
    "        mask = {k: v for k, v in preprocessed_obs.items() if k.startswith(\"mask\")}\n",
    "        if len(mask) == 0:\n",
    "            mask = None\n",
    "        real_actions = actions = player.get_actions(preprocessed_obs, mask=mask)\n",
    "        actions = torch.cat(actions, -1).cpu().numpy()\n",
    "        if is_continuous:\n",
    "            real_actions = torch.stack(real_actions, dim=-1).cpu().numpy()\n",
    "        else:\n",
    "            real_actions = torch.stack([real_act.argmax(dim=-1) for real_act in real_actions], dim=-1).cpu().numpy()\n",
    "\n",
    "    step_data[\"actions\"] = actions.reshape((1, cfg.env.num_envs, -1))\n",
    "    rb_initial.add(step_data, validate_args=cfg.buffer.validate_args)\n",
    "\n",
    "    next_obs, rewards, dones, truncated, infos = envs.step(real_actions.reshape(envs.action_space.shape))\n",
    "    rewards = np.array(rewards).reshape((1, cfg.env.num_envs, -1))\n",
    "    dones = np.logical_or(dones, truncated).astype(np.uint8).reshape((1, cfg.env.num_envs, -1))\n",
    "\n",
    "    step_data[\"is_first\"] = np.zeros_like(step_data[\"dones\"])\n",
    "    if \"restart_on_exception\" in infos:\n",
    "        for i, agent_roe in enumerate(infos[\"restart_on_exception\"]):\n",
    "            if agent_roe and not dones[i]:\n",
    "                last_inserted_idx = (rb_initial.buffer[i]._pos - 1) % rb_initial.buffer[i].buffer_size\n",
    "                rb_initial.buffer[i][\"dones\"][last_inserted_idx] = np.ones_like(\n",
    "                    rb_initial.buffer[i][\"dones\"][last_inserted_idx]\n",
    "                )\n",
    "                rb_initial.buffer[i][\"is_first\"][last_inserted_idx] = np.zeros_like(\n",
    "                    rb_initial.buffer[i][\"is_first\"][last_inserted_idx]\n",
    "                )\n",
    "                step_data[\"is_first\"][i] = np.ones_like(step_data[\"is_first\"][i])\n",
    "\n",
    "    real_next_obs = copy.deepcopy(next_obs)\n",
    "    if \"final_observation\" in infos:\n",
    "        for idx, final_obs in enumerate(infos[\"final_observation\"]):\n",
    "            if final_obs is not None:\n",
    "                for k, v in final_obs.items():\n",
    "                    real_next_obs[k][idx] = v\n",
    "\n",
    "    for k in obs_keys:\n",
    "        step_data[k] = next_obs[k][np.newaxis]\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    rewards = rewards.reshape((1, cfg.env.num_envs, -1))\n",
    "    step_data[\"dones\"] = dones.reshape((1, cfg.env.num_envs, -1))\n",
    "    step_data[\"rewards\"] = clip_rewards_fn(rewards)\n",
    "    step_data[\"rewards\"] = clip_rewards_fn(rewards)\n",
    "    step_data[\"stochastic_state\"] = player.stochastic_state.detach().cpu().numpy()\n",
    "    step_data[\"recurrent_state\"] = player.recurrent_state.detach().cpu().numpy()\n",
    "    dones_idxes = dones.nonzero()[0].tolist()\n",
    "    reset_envs = len(dones_idxes)\n",
    "    if reset_envs > 0:\n",
    "        reset_data = {}\n",
    "        for k in obs_keys:\n",
    "            reset_data[k] = (real_next_obs[k][dones_idxes])[np.newaxis]\n",
    "        reset_data[\"dones\"] = np.ones((1, reset_envs, 1))\n",
    "        reset_data[\"actions\"] = np.zeros((1, reset_envs, np.sum(actions_dim)))\n",
    "        reset_data[\"rewards\"] = step_data[\"rewards\"][:, dones_idxes]\n",
    "        reset_data[\"is_first\"] = np.zeros_like(reset_data[\"dones\"])\n",
    "        rb_initial.add(reset_data, dones_idxes, validate_args=cfg.buffer.validate_args)\n",
    "\n",
    "        # Reset already inserted step data\n",
    "        step_data[\"rewards\"][:, dones_idxes] = np.zeros_like(reset_data[\"rewards\"])\n",
    "        step_data[\"dones\"][:, dones_idxes] = np.zeros_like(step_data[\"dones\"][:, dones_idxes])\n",
    "        step_data[\"is_first\"][:, dones_idxes] = np.ones_like(step_data[\"is_first\"][:, dones_idxes])\n",
    "        player.init_states(dones_idxes)\n",
    "\n",
    "    ## Save the recurrent and stochastic latent states for the imagination phase\n",
    "    if i == initial_steps - imagination_steps - 1:\n",
    "        stochastic_state = player.stochastic_state.clone()\n",
    "        recurrent_state = player.recurrent_state.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagination and Reconstruction\n",
    "\n",
    "This is where the imagination and reconstruction phase takes place. You can decide whether to sample actions (as during training) or use the actions collected during training.\n",
    "\n",
    "The reconstructed steps are the same as those imagined, for easier comparison.\n",
    "\n",
    "In the `rb_imagination` buffer are stored also the actions: you can compare them with the ones in the `rb_intial` to check whether or not the agent takes the same actions it has imagined (this makes sense only if `imagine_actions=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deciede if you want to take the actions from the buffer\n",
    "# (i.e., the actions actually played by the agent)\n",
    "# or imagine them and compare with the actions actually played by the agent\n",
    "imagine_actions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagination / reconstruction obs process\n",
    "imagined_latent_states = torch.cat((stochastic_state, recurrent_state), -1)\n",
    "step_data = {}\n",
    "reconstruced_step_data = {}\n",
    "with torch.no_grad():\n",
    "    for i in range(imagination_steps):\n",
    "        if imagine_actions:\n",
    "            # imagined actions\n",
    "            actions = actor(imagined_latent_states.detach())[0][0]\n",
    "        else:\n",
    "            # actions actually played by the agent\n",
    "            actions = torch.tensor(\n",
    "                rb_initial[\"actions\"][-imagination_steps + i],\n",
    "                device=fabric.device,\n",
    "                dtype=torch.float32,\n",
    "            )[None]\n",
    "\n",
    "        # imagination step\n",
    "        stochastic_state, recurrent_state = world_model.rssm.imagination(stochastic_state, recurrent_state, actions)\n",
    "        # update current state\n",
    "        imagined_latent_states = torch.cat((stochastic_state.view(1, 1, -1), recurrent_state), -1)\n",
    "        rec_obs = world_model.observation_model(imagined_latent_states)\n",
    "        step_data[\"rgb\"] = rec_obs[\"rgb\"].unsqueeze(0).detach().cpu().numpy()\n",
    "        step_data[\"actions\"] = actions.unsqueeze(0).detach().cpu().numpy()\n",
    "        rb_imagination.add(step_data)\n",
    "\n",
    "        # reconstruct the observations from the latent states used when interacting with the environment\n",
    "        played_latent_states = torch.cat(\n",
    "            (\n",
    "                torch.tensor(rb_initial[\"stochastic_state\"][-imagination_steps + i], device=fabric.device),\n",
    "                torch.tensor(rb_initial[\"recurrent_state\"][-imagination_steps + i], device=fabric.device),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        rec_obs_played = world_model.observation_model(played_latent_states)\n",
    "        # The decoder has been trained to reconstruct the observations from the latent states in the range [-0.5, 0.5]\n",
    "        # NOTE: Check how the observations are handled in older versions of SheepRL (before 0.5.5)\n",
    "        # if you need to add 0.5 or not (in latest versions it is done automatically by the decoder in its forward method).\n",
    "        reconstruced_step_data[\"rgb\"] = rec_obs_played[\"rgb\"].unsqueeze(0).detach().cpu().numpy() + 0.5\n",
    "        rb_play.add(reconstruced_step_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GIF creation\n",
    "\n",
    "Here the GIFs are created, the same steps are taken into account for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif with the imagined frames (the latent state is computed by the transition model)\n",
    "frames = torch.clamp(torch.tensor(rb_imagination[\"rgb\"][:, 0, 0]), 0, 1).detach()\n",
    "frames = [torchvision.transforms.functional.to_pil_image(f) for f in frames]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(\"imagination.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif with the reconstructed observations (the latent state is computed by the representation model)\n",
    "frames = torch.clamp(torch.tensor(rb_play[\"rgb\"][:, 0]), 0, 1).detach()\n",
    "frames = [torchvision.transforms.functional.to_pil_image(f) for f in frames]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(\"reconstructed_obs.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif with the real observations (provided by the environment)\n",
    "frames = torch.tensor(rb_initial[\"rgb\"][-imagination_steps:, 0])\n",
    "frames = [torchvision.transforms.functional.to_pil_image(f) for f in frames]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(\"real_obs.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
