{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pathlib\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from lightning.fabric import Fabric\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "\n",
    "from sheeprl.algos.dreamer_v3.agent import PlayerDV3, build_agent\n",
    "from sheeprl.data.buffers import SequentialReplayBuffer\n",
    "from sheeprl.utils.env import make_env\n",
    "from sheeprl.utils.utils import dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of your checkpoint\n",
    "ckpt_path = pathlib.Path(\"/path/to/your/checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "fabric = Fabric(accelerator=\"cuda\", devices=1)\n",
    "state = fabric.load(ckpt_path)\n",
    "cfg = dotdict(OmegaConf.to_container(OmegaConf.load(ckpt_path.parent.parent / \"config.yaml\"), resolve=True))\n",
    "cfg.env.num_envs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [\n",
    "        make_env(\n",
    "            cfg,\n",
    "            cfg.seed + 0 * cfg.env.num_envs + i,\n",
    "            0 * cfg.env.num_envs,\n",
    "            \"./imagination\",\n",
    "            \"imagination\",\n",
    "            vector_env_idx=i,\n",
    "        )\n",
    "        for i in range(cfg.env.num_envs)\n",
    "    ]\n",
    ")\n",
    "action_space = envs.single_action_space\n",
    "observation_space = envs.single_observation_space\n",
    "obs_keys = cfg.algo.cnn_keys.encoder + cfg.algo.mlp_keys.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_continuous = isinstance(action_space, gym.spaces.Box)\n",
    "is_multidiscrete = isinstance(action_space, gym.spaces.MultiDiscrete)\n",
    "actions_dim = tuple(\n",
    "    action_space.shape if is_continuous else (action_space.nvec.tolist() if is_multidiscrete else [action_space.n])\n",
    ")\n",
    "world_model, actor, critic, critic_target = build_agent(\n",
    "    fabric,\n",
    "    actions_dim,\n",
    "    is_continuous,\n",
    "    cfg,\n",
    "    observation_space,\n",
    "    state[\"world_model\"],\n",
    "    state[\"actor\"],\n",
    "    state[\"critic\"],\n",
    "    state[\"target_critic\"],\n",
    ")\n",
    "player = PlayerDV3(\n",
    "    world_model.encoder.module,\n",
    "    world_model.rssm,\n",
    "    actor.module,\n",
    "    actions_dim,\n",
    "    cfg.env.num_envs,\n",
    "    cfg.algo.world_model.stochastic_size,\n",
    "    cfg.algo.world_model.recurrent_model.recurrent_state_size,\n",
    "    fabric.device,\n",
    "    cfg.algo.world_model.discrete_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_steps = 200  # set according to your environment\n",
    "imagination_steps = 45  # number of imagination steps\n",
    "clip_rewards_fn = lambda r: torch.tanh(r) if cfg.env.clip_rewards else r\n",
    "rb_imagination = SequentialReplayBuffer(imagination_steps, cfg.env.num_envs)\n",
    "rb_play = SequentialReplayBuffer(imagination_steps, cfg.env.num_envs)\n",
    "rb_initial = SequentialReplayBuffer(initial_steps, cfg.env.num_envs)\n",
    "step_data = {}\n",
    "player.init_states()\n",
    "obs = envs.reset(seed=cfg.seed)[0]\n",
    "for k in obs_keys:\n",
    "    step_data[k] = obs[k][np.newaxis]\n",
    "step_data[\"dones\"] = np.zeros((1, cfg.env.num_envs, 1))\n",
    "step_data[\"rewards\"] = np.zeros((1, cfg.env.num_envs, 1))\n",
    "step_data[\"is_first\"] = np.ones_like(step_data[\"dones\"])\n",
    "step_data[\"stochastic_state\"] = player.stochastic_state.detach().cpu().numpy()\n",
    "step_data[\"recurrent_state\"] = player.recurrent_state.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_list = []\n",
    "# play for `initial_steps` steps\n",
    "for i in range(initial_steps):\n",
    "    with torch.no_grad():\n",
    "        preprocessed_obs = {}\n",
    "        for k, v in obs.items():\n",
    "            preprocessed_obs[k] = torch.as_tensor(v[np.newaxis], dtype=torch.float32, device=fabric.device)\n",
    "            if k in cfg.algo.cnn_keys.encoder:\n",
    "                preprocessed_obs[k] = preprocessed_obs[k] / 255.0\n",
    "        mask = {k: v for k, v in preprocessed_obs.items() if k.startswith(\"mask\")}\n",
    "        if len(mask) == 0:\n",
    "            mask = None\n",
    "        real_actions = actions = player.get_exploration_action(preprocessed_obs, mask)\n",
    "        actions = torch.cat(actions, -1).cpu().numpy()\n",
    "        if is_continuous:\n",
    "            real_actions = torch.cat(real_actions, dim=-1).cpu().numpy()\n",
    "        else:\n",
    "            real_actions = torch.cat([real_act.argmax(dim=-1) for real_act in real_actions], dim=-1).cpu().numpy()\n",
    "\n",
    "    step_data[\"actions\"] = actions.reshape((1, cfg.env.num_envs, -1))\n",
    "    rb_initial.add(step_data, validate_args=cfg.buffer.validate_args)\n",
    "\n",
    "    next_obs, rewards, dones, truncated, infos = envs.step(real_actions.reshape(envs.action_space.shape))\n",
    "    rewards = np.array(rewards).reshape((1, cfg.env.num_envs, -1))\n",
    "    dones = np.logical_or(dones, truncated).astype(np.uint8).reshape((1, cfg.env.num_envs, -1))\n",
    "\n",
    "    step_data[\"is_first\"] = np.zeros_like(step_data[\"dones\"])\n",
    "    if \"restart_on_exception\" in infos:\n",
    "        for i, agent_roe in enumerate(infos[\"restart_on_exception\"]):\n",
    "            if agent_roe and not dones[i]:\n",
    "                last_inserted_idx = (rb_initial.buffer[i]._pos - 1) % rb_initial.buffer[i].buffer_size\n",
    "                rb_initial.buffer[i][\"dones\"][last_inserted_idx] = np.ones_like(\n",
    "                    rb_initial.buffer[i][\"dones\"][last_inserted_idx]\n",
    "                )\n",
    "                rb_initial.buffer[i][\"is_first\"][last_inserted_idx] = np.zeros_like(\n",
    "                    rb_initial.buffer[i][\"is_first\"][last_inserted_idx]\n",
    "                )\n",
    "                step_data[\"is_first\"][i] = np.ones_like(step_data[\"is_first\"][i])\n",
    "\n",
    "    real_next_obs = copy.deepcopy(next_obs)\n",
    "    if \"final_observation\" in infos:\n",
    "        for idx, final_obs in enumerate(infos[\"final_observation\"]):\n",
    "            if final_obs is not None:\n",
    "                for k, v in final_obs.items():\n",
    "                    real_next_obs[k][idx] = v\n",
    "\n",
    "    for k in obs_keys:\n",
    "        step_data[k] = next_obs[k][np.newaxis]\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    rewards = rewards.reshape((1, cfg.env.num_envs, -1))\n",
    "    step_data[\"dones\"] = dones.reshape((1, cfg.env.num_envs, -1))\n",
    "    step_data[\"rewards\"] = clip_rewards_fn(rewards)\n",
    "    step_data[\"rewards\"] = clip_rewards_fn(rewards)\n",
    "    step_data[\"stochastic_state\"] = player.stochastic_state.detach().cpu().numpy()\n",
    "    step_data[\"recurrent_state\"] = player.recurrent_state.detach().cpu().numpy()\n",
    "    dones_idxes = dones.nonzero()[0].tolist()\n",
    "    reset_envs = len(dones_idxes)\n",
    "    if reset_envs > 0:\n",
    "        reset_data = {}\n",
    "        for k in obs_keys:\n",
    "            reset_data[k] = (real_next_obs[k][dones_idxes])[np.newaxis]\n",
    "        reset_data[\"dones\"] = np.ones((1, reset_envs, 1))\n",
    "        reset_data[\"actions\"] = np.zeros((1, reset_envs, np.sum(actions_dim)))\n",
    "        reset_data[\"rewards\"] = step_data[\"rewards\"][:, dones_idxes]\n",
    "        reset_data[\"is_first\"] = np.zeros_like(reset_data[\"dones\"])\n",
    "        rb_initial.add(reset_data, dones_idxes, validate_args=cfg.buffer.validate_args)\n",
    "\n",
    "        # Reset already inserted step data\n",
    "        step_data[\"rewards\"][:, dones_idxes] = np.zeros_like(reset_data[\"rewards\"])\n",
    "        step_data[\"dones\"][:, dones_idxes] = np.zeros_like(step_data[\"dones\"][:, dones_idxes])\n",
    "        step_data[\"is_first\"][:, dones_idxes] = np.ones_like(step_data[\"is_first\"][:, dones_idxes])\n",
    "        player.init_states(dones_idxes)\n",
    "    if i == initial_steps - imagination_steps - 1:\n",
    "        stochastic_state = player.stochastic_state.clone()\n",
    "        recurrent_state = player.recurrent_state.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagination / reconstruction obs process\n",
    "imagined_latent_states = torch.cat((stochastic_state, recurrent_state), -1)\n",
    "step_data = {}\n",
    "reconstruced_step_data = {}\n",
    "with torch.no_grad():\n",
    "    for i in range(imagination_steps):\n",
    "        # deciede if you want to take the actions from the buffer\n",
    "        # (i.e., the actions actually played by the agent)\n",
    "        # or imagine them and compare with the actions actually played by the agent\n",
    "\n",
    "        # actions actually played by the agent\n",
    "        # actions = torch.tensor(\n",
    "        #     rb_initial[\"actions\"][-imagination_steps + i],\n",
    "        #     device=fabric.device,\n",
    "        #     dtype=torch.float32,\n",
    "        # )[None]\n",
    "\n",
    "        # imagined actions\n",
    "        actions = actor(imagined_latent_states.detach())[0][0]\n",
    "\n",
    "        # imagination step\n",
    "        imagined_stochastic_state, recurrent_state = world_model.rssm.imagination(\n",
    "            stochastic_state, recurrent_state, actions\n",
    "        )\n",
    "        # update current state\n",
    "        imagined_latent_states = torch.cat((imagined_stochastic_state.view(1, 1, -1), recurrent_state), -1)\n",
    "        rec_obs = world_model.observation_model(imagined_latent_states)\n",
    "        step_data[\"rgb\"] = rec_obs[\"rgb\"].unsqueeze(0).detach().cpu().numpy()\n",
    "        step_data[\"actions\"] = actions.unsqueeze(0).detach().cpu().numpy()\n",
    "        rb_imagination.add(step_data)\n",
    "\n",
    "        # reconstruct the observations from the latent states used when interacting with the environment\n",
    "        played_latent_states = torch.cat(\n",
    "            (\n",
    "                torch.tensor(rb_initial[\"stochastic_state\"][-imagination_steps + i], device=fabric.device),\n",
    "                torch.tensor(rb_initial[\"recurrent_state\"][-imagination_steps + i], device=fabric.device),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        rec_obs_played = world_model.observation_model(played_latent_states)\n",
    "        reconstruced_step_data[\"rgb\"] = rec_obs_played[\"rgb\"].unsqueeze(0).detach().cpu().numpy()\n",
    "        rb_play.add(reconstruced_step_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif with the imagined frames (the latent state is computed by the transition model)\n",
    "frames = torch.clamp(torch.tensor(rb_imagination[\"rgb\"][:, 0, 0]), 0, 1).detach()\n",
    "frames = [torchvision.transforms.functional.to_pil_image(f) for f in frames]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(\"imagination.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif with the reconstructed observations (the latent state is computed by the representation model)\n",
    "frames = torch.clamp(torch.tensor(rb_play[\"rgb\"][:, 0]), 0, 1).detach()\n",
    "frames = [torchvision.transforms.functional.to_pil_image(f) for f in frames]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(\"reconstructed_obs.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif with the real observations (provided by the environment)\n",
    "frames = torch.tensor(rb_initial[\"rgb\"][-imagination_steps:, 0])\n",
    "frames = [torchvision.transforms.functional.to_pil_image(f) for f in frames]\n",
    "frame_one = frames[0]\n",
    "frame_one.save(\"real_obs.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
